---
title: "Empirical alts analysis"
author: "Ben Peloquin"
date: "June 29, 2016"
output:
  html_document:
    toc: true
    toc_depth: 4
---

```{r envir, echo=FALSE}
rm(list=ls())
setwd("~/Desktop/Projects/scalar_alts/")
home_path <- "~/Desktop/Projects/scalar_alts/"
set.seed(1)
```


```{r load_packages, echo=FALSE, message=FALSE}
library(lsa)
library(stringr)
library(rjson)
library(igraph)
library(ggplot2)
library(dplyr)
library(tidyr)
```

```{r helpers}
alternatives_store <-
  data.frame("bad"="bad_terrible",
             "terrible"="bad_terrible",
             "disliked"="disliked_hated",
             "hated"="disliked_hated",
             "good"="good_excellent",
             "excellent"="good_excellent",
             "liked"="liked_loved",
             "loved"="liked_loved",
             "memorable"="memorable_unforgettable",
             "unforgettable"="memorable_unforgettable",
             "special"="special_unique",
             "unique"="special_unique",
             "low"="low_high",
             "high"="low_high")
SCALES <- c("bad_terrible", "good_excellent", "liked_loved", "memorable_unforgettable", "special_unique")
DOMAINS <- c("album", "book", "game", "movie", "play", "restaurant")
inGlobalEnv <- function(item) {
  item %in% ls(envir = .GlobalEnv)
}
```

Create word-doc matrix for a given domain. This function is used to output the domain .csv files which were initially intended to be used in the jupyter notebook.
```{r data_processing_fn}
create_wd <- function(d, curr_domain) {
  if (!inGlobalEnv("alternatives_store")) stop("`alternatives_store` must be in global env")
  
  d1 <- d %>%
    gather(scalar, response) %>%
    rowwise %>%
    mutate(scalar_pair=alternatives_store[[scalar]]) %>%
    group_by(scalar_pair, response) %>%
    summarise(counts=n()) %>%
    mutate(domain=curr_domain) %>%
    arrange(scalar_pair, -counts) %>%
    spread(scalar_pair, counts) %>%
    select(-domain)
  
  ## NAs to 0
  d1[is.na(d1)] <- 0
  ## Change names
  names(d1) <- paste0(curr_domain, "_", names(d1))
  
  d1
}
```

Read in data and create word-doct matrices.
```{r load_data, warning=FALSE, message=FALSE}
## Raw data
album_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/album_corrected.json")))
book_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/book_corrected2.json")))
game_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/game_corrected.json")))
movie_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/movie_corrected.json")))
play_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/play_corrected.json")))
restaurant_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/restaurant_corrected.json")))

## Create wd matrices
album_wd      <- create_wd(album_d, "album")
book_wd       <- create_wd(book_d, "book")
game_wd       <- create_wd(game_d, "game")
movie_wd      <- create_wd(movie_d, "movie")
play_wd       <- create_wd(book_d, "play")
restaurant_wd <- create_wd(restaurant_d, "restaurant")

## Write wd matrices
# write.csv(album_wd, "analysis/wd_matrices/album_wd.csv")
# write.csv(book_wd, "analysis/wd_matrices/book_wd.csv")
# write.csv(game_wd, "analysis/wd_matrices/game_wd.csv")
# write.csv(movie_wd, "analysis/wd_matrices/movie_wd.csv")
# write.csv(play_wd, "analysis/wd_matrices/play_wd.csv")
# write.csv(restaurant_wd, "analysis/wd_matrices/restaurant_wd.csv")
```

# Prelim analysis

## Combine data from six domains

Create a data from for each domain with cols `scalar_pair` (i.e. "liked-loved"), `response` (i.e. empirical alt such as "hated"), `counts` (i.e. the number of times "hated" was generated by for the scale "liked-loved" and domain), `domain` current domain (i.e. "album").
```{r create_dfs, warning=FALSE, message=FALSE}
album_df <- album_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="album") %>%
  arrange(scalar_pair, -counts)

book_df <- book_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="book") %>%
  arrange(scalar_pair, -counts)

game_df <- game_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="game") %>%
  arrange(scalar_pair, -counts)

movie_df <- movie_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="movie") %>%
  arrange(scalar_pair, -counts)

play_df <- play_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="play") %>%
  arrange(scalar_pair, -counts)

restaurant_df <- restaurant_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="restaurant") %>%
  arrange(scalar_pair, -counts)

```

Rbind all data frames
```{r create_aggregate_df}
aggregate_df <- rbind(album_df, book_df, game_df, movie_df, play_df, restaurant_df) %>%
  filter(scalar_pair != "low_high") %>%
  ungroup()
```

## Looking at alternatives by scalar pair and domain

Plot any alternative that was produced at least ten times for each scalar pair in each domain.  
```{r count_plots1, warning=FALSE, message=FALSE}
aggregate_df %>%
  filter(counts >= 10) %>%
  ggplot(aes(x=reorder(response, response), y=counts, fill=domain)) +
    geom_bar(position="dodge", stat="identity") +
    facet_wrap(~scalar_pair, scales="free") +
    ylim(c(0, 80)) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r within_scalar_pairs_plot1}
aggregate_df %>%
  group_by(response) %>%
  summarise(total_counts=sum(counts)) %>%
  arrange(-total_counts)

aggregate_df %>%
  group_by(scalar_pair, response) %>%
  summarise(within_total_counts=sum(counts)) %>%
  filter(within_total_counts > 25) %>%
  ggplot(aes(x=reorder(response, within_total_counts), y=within_total_counts, col=scalar_pair)) +
  geom_line(group=1) +
  facet_wrap(~scalar_pair, scale="free") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


## Top alts aggregate

Aggregate across domains
```{r counts_over_domains}
top_aggregate_counts <- aggregate_df %>%
  filter(scalar_pair != "low_high") %>%
  mutate(domain=as.factor(domain)) %>%
  group_by(scalar_pair, response) %>%
  summarise(agg_count=sum(counts)) %>%
  arrange(scalar_pair, desc(agg_count)) %>%
  filter(agg_count >= 10)
```

Plot top alts over domains
```{r top_alts_plot}
ggplot(top_aggregate_counts, aes(x=reorder(response, agg_count), y=agg_count)) +
  geom_bar(position="dodge", stat="identity") +
  facet_wrap(~scalar_pair, scales="free") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r picking_alts_sets, eval=FALSE}
top_aggregate_counts %>%
  filter(scalar_pair == "bad_terrible") %>%
  arrange(desc(agg_count)) %>%
  View
```


## Top alts by domain and scale

Examine top alternatives by domain and scale.
```{r top_alts_table, results='asis', eval=FALSE}
top_alts <- aggregate_df %>%
  filter(scalar_pair != "low_high") %>%
  mutate(domain=as.factor(domain)) %>%
  group_by(scalar_pair, domain) %>%
  arrange(desc(counts)) %>%
  slice(1:6)

knitr::kable(top_alts)

## Can we look at the aggregate top alts?
length(unique(top_alts$response))

# top_alts_tab <- xtable::xtable(top_alts)
# print(top_alts_tab, type="html")
```

## Distributions analysis

Number of unique items generated by scalar pair by domain
```{r count_plots2, warning=FALSE, message=FALSE, eval=FALSE}
aggregate_df %>%
  group_by(scalar_pair, domain) %>%
  summarize(num_unique=length(unique(response))) %>%
  ggplot(aes(x=reorder(scalar_pair, num_unique), y=num_unique, fill=domain)) +
    geom_bar(position="dodge", stat="identity") +
    xlab("scalar pair") +
    ylab("number of unique alternatives") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

-- `liked-loved` and `disliked_hated` tend to have fewer unique items overall. This is likely because alternative sets are dominated by `liked`, `loved`, `disliked` and `hated`.

How do alts generated by domain look different for each scalar pair?
```{r count_plots3, warning=FALSE, message=FALSE, eval=FALSE}
aggregate_df %>%
  group_by(scalar_pair, domain) %>%
  summarize(num_unique=length(unique(response))) %>%
  ggplot(aes(x=reorder(as.character(domain), as.character(domain)), y=num_unique, col=scalar_pair)) +
    # geom_bar(position="dodge", stat="identity") +
    xlab("scalar pair") +
    ylab("number of unique alternatives") +
    geom_line(group=1) +
    facet_grid(~scalar_pair) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

-- Clearly `memorable-unforgettable` and `special-unique` are different beasts than the others (intuitively they are some how less "scalar"?). Can we intrepret the peaks and valleys as a degree of appropriateness in a given domain?  More peakiness indicates more uncertainty around the alternative set (hence more alts generated...).

# Bootstrap sim measures

## Bootstrap helpers
```{r bootstrap_helpers}
calc_sim <- function(measure, v1, v2) {
  measure(v1, v2)
}

boot_vec <- function(countsVec, n = 1) {
  as.vector(rmultinom(n, sum(countsVec), countsVec))
}

## bstrap_sims()
## -------------
## boostrap estimates for some similarity function `fn`
## and two numeric vectors `v1`, `v2`
##
bstrap_sims <- function(fn, v1, v2,  n_sims=100) {
  sapply(
    seq(1, n_sims),
    function(s) {
      fn(boot_vec(v1), boot_vec(v2))
    })
}
# bstrap_sims(cosine, a, b)


## bstrap_similarity()
## -------------------
## see lsa::cosine() for inspiration
##
bstrap_similarity <- function(m, measure, v2=NA, n_sims=100) {
  
  ## Vectors
  if (is.vector(m) & is.vector(v2)) {
    warning("Identified two vectors, running vector operations")
    sims <- bstrap_sims(measure, m, v2, n_sims)
    return(list("mean_measure"=mean(sims), "sd_measure"=sd(sims)))
  }

  ## Matrix
  warning("Identified a matrix, processing over matrix columns")
  sim_matrix <- array(0, c(ncol(m), ncol(m)))
  sd_matrix <- array(0, c(ncol(m), ncol(m)))
  col_names = colnames(m)
  dimnames(sim_matrix) = list(col_names, col_names)
  dimnames(sd_matrix) = list(col_names, col_names)
  ## Just calcs upper right
  # for (i in 1:ncol(m)) {
  #     for (j in 1:ncol(m)) {
  #       # v1 <- m[,i]
  #       # v2 <- m[,j]
  #       sims <- bstrap_sims(measure,  as.integer(m[[i]]), as.integer(m[[j]]), n_sims)
  #       sim_matrix[i, j] = mean(sims)
  #       sd_matrix[i, j] = sd(sims)
  #     }
  # }
  for (i in 1:ncol(m)) {
      for (j in 1:i) {
        # v1 <- m[,i]
        # v2 <- m[,j]
        sims <- bstrap_sims(measure,  as.integer(m[[i]]), as.integer(m[[j]]), n_sims)
        sim_matrix[i, j] = mean(sims)
        sd_matrix[i, j] = sd(sims)
      }
  }
  sim_diag <- diag(sim_matrix)
  sim_matrix <- sim_matrix + t(sim_matrix)
  diag(sim_matrix) <- sim_diag
  
  sd_diag <- diag(sd_matrix)
  sd_matrix <- sd_matrix + t(sd_matrix)
  diag(sd_matrix) <- sd_diag
  
  list("sim_matrix"=as.matrix(sim_matrix), "sd_matrix"=as.matrix(sd_matrix))
}
```

## JSD helpers

Calculating JSD requires non-zero entries so I'm smoothing with an alpha 1/total number of items.
```{r jsd_helpers}
## smooth_vec()
## ------------
## add 'alpha' smoothing to numeric vector
##
smooth_vec <- function(v, alpha=1/sum(v)) {
  sapply(v, function(i) i + alpha)
}

## norm_vec()
## ----------
## normalize vector
##
norm_vec <- function(v) {
  v / sum(v)
}

## kld()
## ----
## D_{kl} = \sum_{i}P(i) * log(\frac{P(i)}{Q(i)})
##
kld <- function(v1, v2) {
  sum(v1 * log(v1 / v2))
}

## jsd()
## -----
## Jensen-Shannon Distance        :: \sqrt(J-S Divergence)
## JSD (Jensen-Shannon divergence :: JSD(P || Q) = 0.5 * D_{kl}(P||M) + 0.5 * D_{kl}(Q||M)
jsd <- function(v1, v2, smooth=TRUE) {
  if (length(v1) != length(v2)) stop("vectors are different lenghts")
  if (smooth) {
    v1_norm <- norm_vec(smooth_vec(v1))
    v2_norm <- norm_vec(smooth_vec(v2))
  }
  else {
    v1_norm <- norm_vec(v1)
    v2_norm <- norm_vec(v2)
  }
  
  m <- (v1_norm + v2_norm) / 2
  js_divergence <- 0.5 * kld(v1_norm, m) + 0.5 * kld(v2_norm, m)
  return(sqrt(js_divergence))
}
```

## MF suggested analysis

### First compare for one scale, two domains

For `good_excellent` look at `album` and `book` domain.
```{r work_space, eval=FALSE}
ge_album_book <- aggregate_df %>% 
  filter(scalar_pair == "good_excellent",
         (domain == "album" |  domain == "book"))

ge_album_book_wide <- ge_album_book %>%
  spread(domain, counts)
ge_album_book_wide[is.na(ge_album_book_wide)] <- 0

## here's our JS distance

```

Permute between domains
```{r permute_boot_jsd}
booted_jsd <- function(v1, v2, n=1000) {
  permute_vecs <- function(v1, v2) {
    df <- data.frame(v1=v1, v2=v2)
    n <- nrow(df)
    df <- df %>%
      mutate(permute_from=rbinom(n, 1, 0.5)) %>%
      rowwise %>%
      mutate(v1_p=ifelse(permute_from==1, v1, v2),
             v2_p=ifelse(permute_from==1, v2, v1))
    df[, c(4, 5)]
  }
  df <- data.frame(v1=v1, v2=v2)
  res <- parallel::mclapply(seq(1, n), FUN=function(i) {
    curr_vec_df <- permute_vecs(df[["v1"]], df[["v2"]])
    jsd(curr_vec_df[[1]], curr_vec_df[[2]])
      }, mc.cores=parallel::detectCores()) %>%
    unlist()
  res
}
```

Within scale variation...
```{r run_permuted_booted_jsd, results='asis'}
## Read in cached data
d_res <- read.csv("/Users/benpeloquin/Desktop/Projects/scalar_alts/analysis/cache/booted_permuted_jsd_ests_n5000.csv")
d_booted <- read.csv("/Users/benpeloquin/Desktop/Projects/scalar_alts/analysis/cache/booted_permuted_jsd_n5000.csv")

if (!(inGlobalEnv("d_booted") & inGlobalEnv("d_res"))) {
  NUM_SIMS <- 5000
  d_res <- data.frame(scale=c(), domain1=c(), domain2=c(), jsd_est=c(), threshold=c())
  d_booted <- data.frame(scale=c(), domain1=c(), domain2=c(), jsd_est=c())
  for (curr_scale in SCALES) {
    curr_scale_df <- aggregate_df %>% filter(scalar_pair==curr_scale)
    for (i in 1:(length(DOMAINS)-1)) {
      for (j in i:length(DOMAINS)) {
        if (i == j) next
        
        curr_scale_df2 <- curr_scale_df %>% 
          filter(domain == DOMAINS[i] |  domain == DOMAINS[j])
        curr_scale_df_wide <- curr_scale_df2 %>%
          spread(domain, counts)
        curr_scale_df_wide[is.na(curr_scale_df_wide)] <- 0
        domain1 <- DOMAINS[i]
        domain2 <- DOMAINS[j]
        v1 <- curr_scale_df_wide[[domain1]]
        v2 <- curr_scale_df_wide[[domain2]]
        # v1 <- curr_scale_df[curr_scale_df$domain==DOMAINS[i], ]$counts
        # v2 <- curr_scale_df[curr_scale_df$domain==DOMAINS[j], ]$counts
        actual_jsd <- jsd(v1, v2)
        boot_jsd <- booted_jsd(v1, v2, NUM_SIMS)
        d_booted <- rbind(d_booted, data.frame(scale=rep(curr_scale, NUM_SIMS),
                                               domain1=rep(domain1, NUM_SIMS),
                                               domain2=rep(domain2, NUM_SIMS),
                                               jsd_est=boot_jsd))
        thresh <- quantile(boot_jsd, probs=c(0.975))
        d_res <- rbind(d_res,
                       data.frame(scale=curr_scale,
                                  domain1=domain1,
                                  domain2=domain2,
                                  jsd_est=actual_jsd,
                                  threshold=thresh))
      }
    }
  }  
}


# write.csv(d_booted, "/Users/benpeloquin/Desktop/Projects/scalar_alts/analysis/cache/booted_permuted_jsd_n5000.csv")
# write.csv(d_res, "/Users/benpeloquin/Desktop/Projects/scalar_alts/analysis/cache/booted_permuted_jsd_ests_n5000.csv")
d_res %>%
  mutate(diff=jsd_est-threshold,
         positive_dff=diff>0) %>% 
  knitr::kable()
```

```{r permuted_booted_jsd_curves}
d_booted %>% 
  filter(domain1=="album", domain2=="book") %>%
  ggplot(aes(x=jsd_est)) +
    geom_density() +
    facet_grid(~scale, scales="free") +
    ggtitle("album vs book\nbootstrapped jsd estimates\nn=5000") +
    theme_bw() +
    theme(axis.text.x = element_text(size=5, angle = 90, hjust = 1))
```

```{r permuted_booted_jsd_curves2}
d_booted %>% 
  filter(scale=="liked_loved") %>%
  ggplot(aes(x=jsd_est)) +
    geom_density() +
    facet_grid(~domain1+domain2, scales="free") +
    ggtitle("scale='liked_loved'\nalbum vs book\nbootstrapped jsd estimates\nn=5000") +
    theme_bw() +
    theme(axis.text.x = element_text(size=5, angle = 90, hjust = 1))
```

## Aggregate JSD across scales -- look at domains

Which domains are most different from eachother?
```{r create_domains_wd}
domains_df <- aggregate_df %>%
  select(domain, response, counts) %>%
  group_by(domain, response) %>%
  summarise(total_counts=sum(counts))
domains_wd <- domains_df %>% spread(domain, total_counts)
domains_wd[is.na(domains_wd)] <- 0
```

```{r run_jsd_on_domains}
jsd_domains <- bstrap_similarity(domains_wd[,-1], jsd, n_sims=100)
jsd_domains_est <- jsd_domains[[1]]
jsd_domains_df <- jsd_domains_est %>% data.frame() %>%
  mutate(domain1=row.names(.)) %>%
  gather(key=domain2, value=jsd, -c(domain1))
```

```{r domains_heatmap}
ggplot(jsd_domains_df, aes(x=domain1, y=domain2)) + 
  geom_tile(aes(fill=jsd), col="white") +
  scale_fill_gradient(low="white", high="steelblue")
```

This is suprisingly unhelpful... But if we look really closely it looks like `restaurant` and `book` are pretty different. Can we confirm with some of the plots from above?

Just looking at the table
```{r}
knitr::kable(jsd_domains_df)
```

So yes, the largest Jensen-Shannon distance is between `restaurant` and `book`.

## Aggregate JSD across domains -- look at scales

Which scales are most different from eachother?
```{r create_scales_wd}
scales_df <- aggregate_df %>%
  select(scalar_pair, response, counts) %>%
  group_by(scalar_pair, response) %>%
  summarise(total_counts=sum(counts))
scales_wd <- scales_df %>% spread(scalar_pair, total_counts)
scales_wd[is.na(scales_wd)] <- 0
```

```{r run_jsd_on_scales}
jsd_scales <- bstrap_similarity(scales_wd[,-1], jsd, n_sims=100)
jsd_scale_est <- jsd_scales[[1]]
jsd_scale_df <- jsd_scale_est %>% data.frame() %>%
  mutate(scale1=row.names(.)) %>%
  gather(key=scale2, value=jsd, -c(scale1))
```

```{r scales_heatmap}
ggplot(jsd_scale_df, aes(x=scale1, y=scale2)) + 
  geom_tile(aes(fill=jsd), col="white") +
  scale_fill_gradient(low="white", high="steelblue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Just looking at the table
```{r}
knitr::kable(jsd_scale_df)
```

```{r combining_scale_domain_df}
jsd_cmbd_df <- read.csv("/Users/benpeloquin/Desktop/Projects/scalar_alts/analysis/cache/jsd_cmbd_df.csv")
if (!inGlobalEnv("jsd_cmbd_df")) {
  cmbd_wd <- aggregate_df %>%
  mutate(cmbd=paste0(domain, "_", scalar_pair)) %>%
  select(cmbd, response, counts) %>%
  spread(cmbd, counts)
  cmbd_wd[is.na(cmbd_wd)] <- 0
  
  jsd_cmbd <- bstrap_similarity(cmbd_wd[,-1], jsd, n_sims=100)
  jsd_cmbd_est <- jsd_cmbd[[1]]
  jsd_cmbd_df <- jsd_cmbd_est %>% data.frame() %>%
    mutate(item1=row.names(.)) %>%
    gather(key=item2, value=jsd, -c(item1))
  
  write.csv(jsd_cmbd_df, "analysis/cache/jsd_cmbd_df.csv")
}
```

```{r cmbd_heatmap, eval=FALSE}
ggplot(jsd_cmbd_df, aes(x=item1, y=item2)) + 
  geom_tile(aes(fill=jsd), col="white") +
  scale_fill_gradient(low="white", high="steelblue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


# Old analysis, keep for now

## Aggregate JSD across domains

### Data

We're going to collapse over scales, grouping all alts generated for a given scale
```{r total_counts, message=FALSE, warning=FALSE, eval=FALSE}
counts_df <- aggregate_df %>%
  filter(scalar_pair != "low_high") %>%
  spread(domain, counts) %>%
  rowwise %>%
  mutate(total_counts=sum(album, book, game, movie, play, restaurant, na.rm=TRUE)) %>%
  select(scalar_pair, response, total_counts) %>%
  group_by(scalar_pair) %>%
  arrange(scalar_pair, desc(total_counts))
counts_wd <- counts_df %>% spread(scalar_pair, total_counts)
counts_wd[is.na(counts_wd)] <- 0
```

### Plotting JSD
```{r jsd_aggregate, message=FALSE, warning=FALSE, eval=FALSE}
jsd_counts <- bstrap_similarity(counts_wd[,-1], jsd, n_sims=100)
jsd_plot_est <- as.data.frame(jsd_counts[[1]]) %>%
  mutate(x=rownames(.)) %>%
  gather(y, counts, -x)
# jsd_plot_est <- jsd_plot_est[!duplicated(jsd_plot_est$counts), ] ## rm dups
jsd_plot_sd <- as.data.frame(jsd_counts[[2]]) %>%
  mutate(x=rownames(.)) %>%
  gather(y, counts, -x)
# jsd_plot_sd <- jsd_plot_sd[!duplicated(jsd_plot_sd$counts), ] ## rm dups

jsd_plot <- jsd_plot_est
jsd_plot$sd <- jsd_plot_sd$counts
jsd_plot <- jsd_plot %>%
  rowwise %>%
  mutate(err_high=counts + qt(0.975,df=100-1)*sd/sqrt(100),
         err_low=counts - qt(0.975,df=100-1)*sd/sqrt(100))
```

```{r aggregate_jsd_plot1, eval=FALSE}
jsd_plot %>%
  mutate(same=x==y) %>%
  ggplot(aes(x=x, y=counts, fill=y)) +
    geom_bar(position=position_dodge(), stat="identity") +
    geom_errorbar(aes(ymin=err_low, ymax=err_high), width=0.8, position=position_dodge(.9)) +
    facet_wrap(~same) +
  ylab("bootstrapped jensen-shannon distance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Run JSD within scales
```{r jsd_run, message=FALSE, warning=FALSE, eval=FALSE}
jsd_within_scale <- function(scale_name, n_sims=100) {
  if (!inGlobalEnv("aggregate_df")) {
    error("aggregate_df not in global env!")
  }
  
  test_jsd <- aggregate_df %>%
  filter(scalar_pair == scale_name) %>%
  mutate(scalar_pair_domain=paste0(scalar_pair, "_", domain)) %>%
  select(domain, scalar_pair_domain, response, counts)
  test_jsd <- test_jsd[, -1] %>% spread(scalar_pair_domain, counts)
  test_jsd[is.na(test_jsd)] <- 0
  
  ## run bstrapped jsd on matrix
  bstrap_similarity(test_jsd[,-1], jsd, n_sims=n_sims)
}

convert_to_df <- function(m) {
  m %>%
    as.data.frame %>%
    mutate(x=rownames(.)) %>%
    gather(domain, jsd, -x)
}

d_within_scales <- data.frame(x=NA, domain=NA, jsd=NA)
for (scale in alternatives_store) {
  scale <- as.character(scale)
  if (scale != "low_high") {
    d_within_scales <- rbind(d_within_scales,
        convert_to_df(jsd_within_scale(scale, n_sims=100)[[1]]))  
  }
}
d_within_scales <- d_within_scales[-1, ]
```


Modify a little to undo the scalar-pair-domain string I needed to create to create the WD matrix
```{r jsd_withing_scale_data, eval=FALSE}
get_scale_pair_pat <- "_(?:album|book|game|movie|play|restaurant)"
get_domain_pat <- ".+_"

d_practice <- d_within_scales %>%
  mutate(scale1=sub(get_scale_pair_pat, "", x),
         scale2=sub(get_scale_pair_pat, "", domain),
         domain1=sub(get_domain_pat, "", x),
         domain2=sub(get_domain_pat, "", domain)) %>%
  select(scale1, scale2, domain1, domain2, jsd)
```


Why are there two estimates here????
```{r jsd_withing_scale_plot1, eval=FALSE}
ggplot(d_practice, aes(x=domain1, y=jsd, col=scale1)) +
  geom_point(alpha=0.4) +
  facet_grid(~domain2) +
  ylab("bootstrapped jensen-shannon distance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r jsd_withing_scale_plot2, eval=FALSE}
ggplot(d_practice, aes(x=reorder(scale1, jsd), y=jsd, col=domain2)) +
  geom_point(alpha=0.4) +
  # geom_line(group=1) +
  facet_grid(~domain1) +
  ylab("bootstrapped jensen-shannon distance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Network - Aggregate across domains - work from cogsci
```{r}
alts_df <- aggregate_df %>%
  select(response, counts, scalar_pair) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts=sum(counts))

alts_m <- alts_df %>%
  spread(scalar_pair, counts)
alts_m[is.na(alts_m)] <- 0
```

```{r network_stuff}
alts_df_nonzero <- alts_df %>%
  filter(counts > 0) %>%
  ungroup %>%
  mutate(scalar_pair=as.character(scalar_pair))
words <- data.frame(words=c(unique(as.character(alts_df_nonzero$scalar_pair)),
                            unique(as.character(alts_df_nonzero$response))))
relations <- data.frame(from=as.character(alts_df_nonzero$scalar_pair),
                        to=as.character(alts_df_nonzero$response),
                        weight=alts_df_nonzero$counts)
g <- graph_from_data_frame(relations, vertices=words)
## appearance
E(g)$width <- (log(E(g)$weight) + 1)/2
V(g)$size <- 3
E(g)$arrow.mode <- 0
E(g)$color <- "blue"
l <- layout_with_fr(g)
l <- norm_coords(l, ymin=-1, ymax=1, xmin=-1, xmax=1)
# plot(g, vertex.label=NA, rescale=F, layout=l*1.0)
plot(g, vertex.label=NA, layout=layout_nicely)
```


# R version
```{r}
R.Version()
```

