---
title: "Empirical alts analysis"
author: "Ben Peloquin"
date: "June 29, 2016"
output:
  html_document:
    toc: true
    toc_depth: 4
---

```{r envir, echo=FALSE}
rm(list=ls())
setwd("~/Desktop/Projects/scalar_alts/")
home_path <- "~/Desktop/Projects/scalar_alts/"
```


```{r load_packages, echo=FALSE, message=FALSE}
library(lsa)
library(stringr)
library(rjson)
library(ggplot2)
library(dplyr)
library(tidyr)
```

```{r helpers}
## Add domain here
# DOMAIN <- "movie"

alternatives_store <-
  data.frame("bad"="bad_terrible",
             "terrible"="bad_terrible",
             "disliked"="disliked_hated",
             "hated"="disliked_hated",
             "good"="good_excellent",
             "excellent"="good_excellent",
             "liked"="liked_loved",
             "loved"="liked_loved",
             "memorable"="memorable_unforgettable",
             "unforgettable"="memorable_unforgettable",
             "special"="special_unique",
             "unique"="special_unique",
             "low"="low_high",
             "high"="low_high")

inGlobalEnv <- function(item) {
  item %in% ls(envir = .GlobalEnv)
}
```

```{r data_processing_fn}
create_wd <- function(d, curr_domain) {
  if (!inGlobalEnv("alternatives_store")) stop("`alternatives_store` must be in global env")
  
  d1 <- d %>%
    gather(scalar, response) %>%
    rowwise %>%
    mutate(scalar_pair=alternatives_store[[scalar]]) %>%
    group_by(scalar_pair, response) %>%
    summarise(counts = n()) %>%
    mutate(domain=curr_domain) %>%
    arrange(scalar_pair, -counts) %>%
    spread(scalar_pair, counts) %>%
    select(-domain)
  
  ## NAs to 0
  d1[is.na(d1)] <- 0
  ## Change names
  names(d1) <- paste0(curr_domain, "_", names(d1))
  
  d1
}
```

Write wd matrices
```{r load_data, warning=FALSE, message=FALSE}
## Raw data
restaurant_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/restaurant_corrected.json")))
movie_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/movie_corrected.json")))
book_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/book_corrected2.json")))
play_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/play_corrected.json")))
album_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/album_corrected.json")))
game_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/game_corrected.json")))
## Create wd matrices
album_wd <- create_wd(book_d, "album")
book_wd <- create_wd(book_d, "book")
game_wd <- create_wd(book_d, "game")
movie_wd <- create_wd(movie_d, "movie")
play_wd <- create_wd(book_d, "play")
restaurant_wd <- create_wd(restaurant_d, "restaurant")
## Write wd matrices
# write.csv(album_wd, "analysis/wd_matrices/album_wd.csv")
# write.csv(book_wd, "analysis/wd_matrices/book_wd.csv")
# write.csv(game_wd, "analysis/wd_matrices/game_wd.csv")
# write.csv(movie_wd, "analysis/wd_matrices/movie_wd.csv")
# write.csv(play_wd, "analysis/wd_matrices/play_wd.csv")
# write.csv(restaurant_wd, "analysis/wd_matrices/restaurant_wd.csv")
```

# Prelim analysis

## Combine data from six domains
```{r create_dfs, warning=FALSE, message=FALSE}
album_df <- album_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="album") %>%
  arrange(scalar_pair, -counts)

book_df <- book_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="book") %>%
  arrange(scalar_pair, -counts)

game_df <- game_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="game") %>%
  arrange(scalar_pair, -counts)

movie_df <- movie_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="movie") %>%
  arrange(scalar_pair, -counts)

play_df <- play_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="play") %>%
  arrange(scalar_pair, -counts)

restaurant_df <- restaurant_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="restaurant") %>%
  arrange(scalar_pair, -counts)

aggregate_df <- rbind(album_df, book_df, game_df, movie_df, play_df, restaurant_df)
```

## Plot top alternatives 
```{r count_plots, warning=FALSE, message=FALSE}
aggregate_df %>%
  filter(counts >= 10,
         scalar_pair != "low_high") %>%
  ggplot(aes(x=reorder(response, response), y=counts, fill=domain)) +
    geom_bar(position="dodge", stat="identity") +
    facet_wrap(~scalar_pair, scales="free") +
    ylim(c(0, 80)) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Top 5 alts by domain and scale
```{r top_alts_table, results='asis'}
top_alts <- aggregate_df %>%
  filter(scalar_pair != "low_high") %>%
  mutate(domain=as.factor(domain)) %>%
  group_by(scalar_pair, domain) %>%
  arrange(desc(counts)) %>%
  slice(1:4)

top_alts_tab <- xtable::xtable(top_alts)
print(top_alts_tab, type="html")
```

# Bootstrap sim measures

## Bootstrap helpers
```{r bootstrap_helpers}
calc_sim <- function(measure, v1, v2) {
  measure(v1, v2)
}

boot_vec <- function(countsVec, n = 1) {
  as.vector(rmultinom(n, sum(countsVec), countsVec))
}

## bstrap_sims()
## -------------
## boostrap estimates for some similarity function `fn`
## and two numeric vectors `v1`, `v2`
##
bstrap_sims <- function(fn, v1, v2,  n_sims=100) {
  sapply(
    seq(1, n_sims),
    function(s) {
      fn(boot_vec(v1), boot_vec(v2))
    })
}
# bstrap_sims(cosine, a, b)


## bstrap_similarity()
## -------------------
## see lsa::cosine() for inspiration
##
bstrap_similarity <- function(m, measure, v2=NA, n_sims=100) {
  
  ## Vectors
  if (is.vector(m) & is.vector(v2)) {
    warning("Identified two vectors, running vector operations")
    sims <- bstrap_sims(measure, m, v2, n_sims)
    return(list("mean_measure"=mean(sims), "sd_measure"=sd(sims)))
  }

  ## Matrix
  warning("Identified a matrix, processing over matrix columns")
  sim_matrix <- array(0, c(ncol(m), ncol(m)))
  sd_matrix <- array(0, c(ncol(m), ncol(m)))
  col_names = colnames(m)
  dimnames(sim_matrix) = list(col_names, col_names)
  dimnames(sd_matrix) = list(col_names, col_names)
  ## Just calcs upper right
  for (i in 1:ncol(m)) {
      for (j in 1:ncol(m)) {
        # v1 <- m[,i]
        # v2 <- m[,j]
        sims <- bstrap_sims(measure,  as.integer(m[[i]]), as.integer(m[[j]]), n_sims)
        sim_matrix[i, j] = mean(sims)
        sd_matrix[i, j] = sd(sims)
      }
  }
  ## Complete matrix
  # sim_matrix <- sim_matrix + t(sim_matrix)
  # sd_matrix <- sd_matrix + t(sd_matrix)
  # diag(sim_matrix) <- 1
  # diag(sd_matrix) <- 0
  
  list("sim_matrix"=as.matrix(sim_matrix), "sd_matrix"=as.matrix(sd_matrix))
}
```

## JSD helpers
```{r jsd_helpers}
## smooth_vec()
## ------------
## add 'alpha' smoothing to numeric vector
##
smooth_vec <- function(v, alpha=1/sum(v)) {
  sapply(v, function(i) i + alpha)
}

## norm_vec()
## ----------
## normalize vector
##
norm_vec <- function(v) {
  v / sum(v)
}

## kld()
## ----
## D_{kl} = \sum_{i}P(i) * log(\frac{P(i)}{Q(i)})
##
kld <- function(v1, v2) {
  sum(v1 * log(v1 / v2))
}

## jsd()
## -----
## Jensen-Shannon Distance        :: \sqrt(J-S Divergence)
## JSD (Jensen-Shannon divergence :: JSD(P || Q) = 0.5 * D_{kl}(P||M) + 0.5 * D_{kl}(Q||M)
jsd <- function(v1, v2, smooth=TRUE) {
  if (length(v1) != length(v2)) stop("vectors are different lenghts")
  if (smooth) {
    v1_norm <- norm_vec(smooth_vec(v1))
    v2_norm <- norm_vec(smooth_vec(v2))
  }
  else {
    v1_norm <- norm_vec(v1)
    v2_norm <- norm_vec(v2)
  }
  
  m <- (v1_norm + v2_norm) / 2
  js_divergence <- 0.5 * kld(v1_norm, m) + 0.5 * kld(v2_norm, m)
  return(sqrt(js_divergence))
}
```

## JSD across domains

### Data
```{r total_counts}
counts_df <- aggregate_df %>%
  filter(scalar_pair != "low_high") %>%
  spread(domain, counts) %>%
  rowwise %>%
  mutate(total_counts=sum(album, book, game, movie, play, restaurant, na.rm=TRUE)) %>%
  select(scalar_pair, response, total_counts) %>%
  group_by(scalar_pair) %>%
  arrange(scalar_pair, desc(total_counts))
counts_wd <- counts_df %>% spread(scalar_pair, total_counts)
counts_wd[is.na(counts_wd)] <- 0
```

### Plot
```{r jsd_aggregate}
jsd_counts <- bstrap_similarity(counts_wd[,-1], jsd, n_sims=100)
jsd_plot_est <- as.data.frame(jsd_counts[[1]]) %>%
  mutate(x=rownames(.)) %>%
  gather(y, counts, -x)
jsd_plot_sd <- as.data.frame(jsd_counts[[2]]) %>%
  mutate(x=rownames(.)) %>%
  gather(y, counts, -x)
jsd_plot <- jsd_plot_est
jsd_plot$sd <- jsd_plot_sd$counts
jsd_plot <- jsd_plot %>%
  rowwise %>%
  mutate(err_high=counts + qt(0.975,df=100-1)*sd/sqrt(100),
         err_low=counts - qt(0.975,df=100-1)*sd/sqrt(100))

ggplot(jsd_plot, aes(x=x, y=counts, fill=y)) +
  geom_bar(position=position_dodge(), stat="identity") +
    geom_errorbar(aes(ymin=err_low, ymax=err_high), width=0.8, position=position_dodge(.9)) +
  ylab("bootstrapped jensen-shannon distance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Run JSD
```{r jsd_run}
set.seed(1)
## prepare data
pair <- "liked_loved"
test_jsd <- aggregate_df %>%
  filter(scalar_pair == pair) %>%
  mutate(scalar_pair_domain=paste0(scalar_pair, "_", domain)) %>%
  select(domain, scalar_pair_domain, response, counts)
test_jsd <- test_jsd[, -c(1, 2)] %>% spread(scalar_pair_domain, counts)
test_jsd[is.na(test_jsd)] <- 0

## run bstrapped jsd on matri
jsd_matrix <- bstrap_similarity(test_jsd[,-1], jsd, n_sims=100)

jsd_matrix[[1]]

## run bstrapped jsd on two vectors
bstrap_similarity(test_jsd[[2]], v2=test_jsd[[4]], jsd, n_sims=100)
```


# Old code save for now
```{r prelim_process, eval=FALSE}
d2 <- d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(count = n()) %>%
  mutate(domain=DOMAIN) %>%
  arrange(scalar_pair, -count)
names(d2)[3] <- "cnt"

ggplot(d2, aes(x=reorder(response, response), y=cnt)) +
  geom_bar(position="dodge", stat="identity") +
  facet_wrap(~scalar_pair, scales="free") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r eval=FALSE}
d3 <- d2 %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(count=n()) %>%
  arrange(-count)

d3 %>%
  arrange(scalar_pair, -cnt) %>% View

ggplot(d3, aes(x=reorder(response, response), y=cnt)) +
  geom_bar(position="dodge", stat="identity") +
  facet_wrap(~scalar_pair, scales="free") +
   theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


# Aggregate data and write to .csv
```{r eval=FALSE}
d_wide <- d2 %>%
  spread(scalar_pair, cnt) %>%
  dplyr::select(-domain)

d_wide[is.na(d_wide)] <- 0

write.csv(d_wide, "analysis/restaurant_td_matrix.csv", row.names=FALSE)
```

# R version
```{r}
R.Version()
```

