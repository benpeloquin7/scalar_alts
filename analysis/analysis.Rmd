---
title: "Empirical alts analysis"
author: "Ben Peloquin"
date: "June 29, 2016"
output:
  html_document:
    toc: true
    toc_depth: 4
---

```{r envir, echo=FALSE}
rm(list=ls())
setwd("~/Desktop/Projects/scalar_alts/")
home_path <- "~/Desktop/Projects/scalar_alts/"
set.seed(1)
```


```{r load_packages, echo=FALSE, message=FALSE}
library(lsa)
library(stringr)
library(rjson)
library(ggplot2)
library(dplyr)
library(tidyr)
```

```{r helpers}
alternatives_store <-
  data.frame("bad"="bad_terrible",
             "terrible"="bad_terrible",
             "disliked"="disliked_hated",
             "hated"="disliked_hated",
             "good"="good_excellent",
             "excellent"="good_excellent",
             "liked"="liked_loved",
             "loved"="liked_loved",
             "memorable"="memorable_unforgettable",
             "unforgettable"="memorable_unforgettable",
             "special"="special_unique",
             "unique"="special_unique",
             "low"="low_high",
             "high"="low_high")

inGlobalEnv <- function(item) {
  item %in% ls(envir = .GlobalEnv)
}
```

Create word-doc matrix for a given domain. This function is used to output the domain .csv files which were initially intended to be used in the jupyter notebook.
```{r data_processing_fn}
create_wd <- function(d, curr_domain) {
  if (!inGlobalEnv("alternatives_store")) stop("`alternatives_store` must be in global env")
  
  d1 <- d %>%
    gather(scalar, response) %>%
    rowwise %>%
    mutate(scalar_pair=alternatives_store[[scalar]]) %>%
    group_by(scalar_pair, response) %>%
    summarise(counts=n()) %>%
    mutate(domain=curr_domain) %>%
    arrange(scalar_pair, -counts) %>%
    spread(scalar_pair, counts) %>%
    select(-domain)
  
  ## NAs to 0
  d1[is.na(d1)] <- 0
  ## Change names
  names(d1) <- paste0(curr_domain, "_", names(d1))
  
  d1
}
```

Read in data and create word-doct matrices.
```{r load_data, warning=FALSE, message=FALSE}
## Raw data
album_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/album_corrected.json")))
book_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/book_corrected2.json")))
game_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/game_corrected.json")))
movie_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/movie_corrected.json")))
play_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/play_corrected.json")))
restaurant_d <- as.data.frame(fromJSON(file=paste0(home_path, "analysis/corrected_data/restaurant_corrected.json")))

## Create wd matrices
album_wd      <- create_wd(album_d, "album")
book_wd       <- create_wd(book_d, "book")
game_wd       <- create_wd(game_d, "game")
movie_wd      <- create_wd(movie_d, "movie")
play_wd       <- create_wd(book_d, "play")
restaurant_wd <- create_wd(restaurant_d, "restaurant")

## Write wd matrices
# write.csv(album_wd, "analysis/wd_matrices/album_wd.csv")
# write.csv(book_wd, "analysis/wd_matrices/book_wd.csv")
# write.csv(game_wd, "analysis/wd_matrices/game_wd.csv")
# write.csv(movie_wd, "analysis/wd_matrices/movie_wd.csv")
# write.csv(play_wd, "analysis/wd_matrices/play_wd.csv")
# write.csv(restaurant_wd, "analysis/wd_matrices/restaurant_wd.csv")
```

# Prelim analysis

## Combine data from six domains

Create a data from for each domain with cols `scalar_pair` (i.e. "liked-loved"), `response` (i.e. empirical alt such as "hated"), `counts` (i.e. the number of times "hated" was generated by for the scale "liked-loved" and domain), `domain` current domain (i.e. "album").
```{r create_dfs, warning=FALSE, message=FALSE}
album_df <- album_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="album") %>%
  arrange(scalar_pair, -counts)

book_df <- book_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="book") %>%
  arrange(scalar_pair, -counts)

game_df <- game_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="game") %>%
  arrange(scalar_pair, -counts)

movie_df <- movie_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="movie") %>%
  arrange(scalar_pair, -counts)

play_df <- play_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="play") %>%
  arrange(scalar_pair, -counts)

restaurant_df <- restaurant_d %>%
  gather(scalar, response) %>%
  rowwise %>%
  mutate(scalar_pair=alternatives_store[[scalar]]) %>%
  group_by(scalar_pair, response) %>%
  summarise(counts = n()) %>%
  mutate(domain="restaurant") %>%
  arrange(scalar_pair, -counts)

```

Rbind all data frames
```{r create_aggregate_df}
aggregate_df <- rbind(album_df, book_df, game_df, movie_df, play_df, restaurant_df) %>%
  filter(scalar_pair != "low_high") %>%
  ungroup()
```

## Looking at alternatives by scalar pair and domain

Plot any alternative that was produced at least ten times for each scalar pair in each domain.  
```{r count_plots1, warning=FALSE, message=FALSE}
aggregate_df %>%
  filter(counts >= 10) %>%
  ggplot(aes(x=reorder(response, response), y=counts, fill=domain)) +
    geom_bar(position="dodge", stat="identity") +
    facet_wrap(~scalar_pair, scales="free") +
    ylim(c(0, 80)) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r within_scalar_pairs_plot1}
aggregate_df %>%
  group_by(response) %>%
  summarise(total_counts=sum(counts)) %>%
  arrange(-total_counts)

aggregate_df %>%
  group_by(scalar_pair, response) %>%
  summarise(within_total_counts=sum(counts)) %>%
  filter(within_total_counts > 25) %>%
  ggplot(aes(x=reorder(response, within_total_counts), y=within_total_counts, col=scalar_pair)) +
  geom_line(group=1) +
  facet_wrap(~scalar_pair, scale="free") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


Number of unique items generated by scalar pair by domain
```{r count_plots2, warning=FALSE, message=FALSE}
aggregate_df %>%
  group_by(scalar_pair, domain) %>%
  summarize(num_unique=length(unique(response))) %>%
  ggplot(aes(x=reorder(scalar_pair, num_unique), y=num_unique, fill=domain)) +
    geom_bar(position="dodge", stat="identity") +
    xlab("scalar pair") +
    ylab("number of unique alternatives") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

-- `liked-loved` and `disliked_hated` tend to have fewer unique items overall. This is likely because alternative sets are dominated by `liked`, `loved`, `disliked` and `hated`.

How do alts generated by domain look different for each scalar pair?
```{r count_plots3, warning=FALSE, message=FALSE}
aggregate_df %>%
  group_by(scalar_pair, domain) %>%
  summarize(num_unique=length(unique(response))) %>%
  ggplot(aes(x=reorder(as.character(domain), as.character(domain)), y=num_unique, col=scalar_pair)) +
    # geom_bar(position="dodge", stat="identity") +
    xlab("scalar pair") +
    ylab("number of unique alternatives") +
    geom_line(group=1) +
    facet_grid(~scalar_pair) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

-- Clearly `memorable-unforgettable` and `special-unique` are different beasts than the others (intuitively they are some how less "scalar"?). Can we intrepret the peaks and valleys as a degree of appropriateness in a given domain?  More peakiness indicates more uncertainty around the alternative set (hence more alts generated...).

## Top 5 alts by domain and scale

Examine top alternatives by domain and scale.
```{r top_alts_table, results='asis'}
top_alts <- aggregate_df %>%
  filter(scalar_pair != "low_high") %>%
  mutate(domain=as.factor(domain)) %>%
  group_by(scalar_pair, domain) %>%
  arrange(desc(counts)) %>%
  slice(1:6)

knitr::kable(top_alts)

## Can we look at the aggregate top alts?
length(unique(top_alts$response))

# top_alts_tab <- xtable::xtable(top_alts)
# print(top_alts_tab, type="html")
```

Aggregate across domains
```{r}
top_aggregate_counts <- aggregate_df %>%
  filter(scalar_pair != "low_high") %>%
  mutate(domain=as.factor(domain)) %>%
  group_by(scalar_pair, response) %>%
  summarise(agg_count=sum(counts)) %>%
  arrange(desc(agg_count)) %>%
  slice(1:6)
knitr::kable(top_aggregate_counts)
```


# Bootstrap sim measures

## Bootstrap helpers
```{r bootstrap_helpers}
calc_sim <- function(measure, v1, v2) {
  measure(v1, v2)
}

boot_vec <- function(countsVec, n = 1) {
  as.vector(rmultinom(n, sum(countsVec), countsVec))
}

## bstrap_sims()
## -------------
## boostrap estimates for some similarity function `fn`
## and two numeric vectors `v1`, `v2`
##
bstrap_sims <- function(fn, v1, v2,  n_sims=100) {
  sapply(
    seq(1, n_sims),
    function(s) {
      fn(boot_vec(v1), boot_vec(v2))
    })
}
# bstrap_sims(cosine, a, b)


## bstrap_similarity()
## -------------------
## see lsa::cosine() for inspiration
##
bstrap_similarity <- function(m, measure, v2=NA, n_sims=100) {
  
  ## Vectors
  if (is.vector(m) & is.vector(v2)) {
    warning("Identified two vectors, running vector operations")
    sims <- bstrap_sims(measure, m, v2, n_sims)
    return(list("mean_measure"=mean(sims), "sd_measure"=sd(sims)))
  }

  ## Matrix
  warning("Identified a matrix, processing over matrix columns")
  sim_matrix <- array(0, c(ncol(m), ncol(m)))
  sd_matrix <- array(0, c(ncol(m), ncol(m)))
  col_names = colnames(m)
  dimnames(sim_matrix) = list(col_names, col_names)
  dimnames(sd_matrix) = list(col_names, col_names)
  ## Just calcs upper right
  # for (i in 1:ncol(m)) {
  #     for (j in 1:ncol(m)) {
  #       # v1 <- m[,i]
  #       # v2 <- m[,j]
  #       sims <- bstrap_sims(measure,  as.integer(m[[i]]), as.integer(m[[j]]), n_sims)
  #       sim_matrix[i, j] = mean(sims)
  #       sd_matrix[i, j] = sd(sims)
  #     }
  # }
  for (i in 1:ncol(m)) {
      for (j in 1:i) {
        # v1 <- m[,i]
        # v2 <- m[,j]
        sims <- bstrap_sims(measure,  as.integer(m[[i]]), as.integer(m[[j]]), n_sims)
        sim_matrix[i, j] = mean(sims)
        sd_matrix[i, j] = sd(sims)
      }
  }
  sim_diag <- diag(sim_matrix)
  sim_matrix <- sim_matrix + t(sim_matrix)
  diag(sim_matrix) <- sim_diag
  
  sd_diag <- diag(sd_matrix)
  sd_matrix <- sd_matrix + t(sd_matrix)
  diag(sd_matrix) <- sd_diag
  
  list("sim_matrix"=as.matrix(sim_matrix), "sd_matrix"=as.matrix(sd_matrix))
}
```

## JSD helpers

Calculating JSD requires non-zero entries so I'm smoothing with an alpha 1/total number of items.
```{r jsd_helpers}
## smooth_vec()
## ------------
## add 'alpha' smoothing to numeric vector
##
smooth_vec <- function(v, alpha=1/sum(v)) {
  sapply(v, function(i) i + alpha)
}

## norm_vec()
## ----------
## normalize vector
##
norm_vec <- function(v) {
  v / sum(v)
}

## kld()
## ----
## D_{kl} = \sum_{i}P(i) * log(\frac{P(i)}{Q(i)})
##
kld <- function(v1, v2) {
  sum(v1 * log(v1 / v2))
}

## jsd()
## -----
## Jensen-Shannon Distance        :: \sqrt(J-S Divergence)
## JSD (Jensen-Shannon divergence :: JSD(P || Q) = 0.5 * D_{kl}(P||M) + 0.5 * D_{kl}(Q||M)
jsd <- function(v1, v2, smooth=TRUE) {
  if (length(v1) != length(v2)) stop("vectors are different lenghts")
  if (smooth) {
    v1_norm <- norm_vec(smooth_vec(v1))
    v2_norm <- norm_vec(smooth_vec(v2))
  }
  else {
    v1_norm <- norm_vec(v1)
    v2_norm <- norm_vec(v2)
  }
  
  m <- (v1_norm + v2_norm) / 2
  js_divergence <- 0.5 * kld(v1_norm, m) + 0.5 * kld(v2_norm, m)
  return(sqrt(js_divergence))
}
```

## Aggregate JSD across domains

### Data

We're going to collapse over scales, grouping all alts generated for a given scale
```{r total_counts, message=FALSE, warning=FALSE}
counts_df <- aggregate_df %>%
  filter(scalar_pair != "low_high") %>%
  spread(domain, counts) %>%
  rowwise %>%
  mutate(total_counts=sum(album, book, game, movie, play, restaurant, na.rm=TRUE)) %>%
  select(scalar_pair, response, total_counts) %>%
  group_by(scalar_pair) %>%
  arrange(scalar_pair, desc(total_counts))
counts_wd <- counts_df %>% spread(scalar_pair, total_counts)
counts_wd[is.na(counts_wd)] <- 0
```

### Plotting JSD
```{r jsd_aggregate, message=FALSE, warning=FALSE}
jsd_counts <- bstrap_similarity(counts_wd[,-1], jsd, n_sims=100)
jsd_plot_est <- as.data.frame(jsd_counts[[1]]) %>%
  mutate(x=rownames(.)) %>%
  gather(y, counts, -x)
# jsd_plot_est <- jsd_plot_est[!duplicated(jsd_plot_est$counts), ] ## rm dups
jsd_plot_sd <- as.data.frame(jsd_counts[[2]]) %>%
  mutate(x=rownames(.)) %>%
  gather(y, counts, -x)
# jsd_plot_sd <- jsd_plot_sd[!duplicated(jsd_plot_sd$counts), ] ## rm dups

jsd_plot <- jsd_plot_est
jsd_plot$sd <- jsd_plot_sd$counts
jsd_plot <- jsd_plot %>%
  rowwise %>%
  mutate(err_high=counts + qt(0.975,df=100-1)*sd/sqrt(100),
         err_low=counts - qt(0.975,df=100-1)*sd/sqrt(100))
```

```{r aggregate_jsd_plot1}
jsd_plot %>%
  mutate(same=x==y) %>%
  ggplot(aes(x=x, y=counts, fill=y)) +
    geom_bar(position=position_dodge(), stat="identity") +
    geom_errorbar(aes(ymin=err_low, ymax=err_high), width=0.8, position=position_dodge(.9)) +
    facet_wrap(~same) +
  ylab("bootstrapped jensen-shannon distance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Run JSD within scales
```{r jsd_run, message=FALSE, warning=FALSE}
jsd_within_scale <- function(scale_name, n_sims=100) {
  if (!inGlobalEnv("aggregate_df")) {
    error("aggregate_df not in global env!")
  }
  
  test_jsd <- aggregate_df %>%
  filter(scalar_pair == scale_name) %>%
  mutate(scalar_pair_domain=paste0(scalar_pair, "_", domain)) %>%
  select(domain, scalar_pair_domain, response, counts)
  test_jsd <- test_jsd[, -1] %>% spread(scalar_pair_domain, counts)
  test_jsd[is.na(test_jsd)] <- 0
  
  ## run bstrapped jsd on matrix
  bstrap_similarity(test_jsd[,-1], jsd, n_sims=n_sims)
}

convert_to_df <- function(m) {
  m %>%
    as.data.frame %>%
    mutate(x=rownames(.)) %>%
    gather(domain, jsd, -x)
}

d_within_scales <- data.frame(x=NA, domain=NA, jsd=NA)
for (scale in alternatives_store) {
  scale <- as.character(scale)
  if (scale != "low_high") {
    d_within_scales <- rbind(d_within_scales,
        convert_to_df(jsd_within_scale(scale, n_sims=100)[[1]]))  
  }
}
d_within_scales <- d_within_scales[-1, ]
```


Modify a little to undo the scalar-pair-domain string I needed to create to create the WD matrix
```{r jsd_withing_scale_data}
get_scale_pair_pat <- "_(?:album|book|game|movie|play|restaurant)"
get_domain_pat <- ".+_"

d_practice <- d_within_scales %>%
  mutate(scale1=sub(get_scale_pair_pat, "", x),
         scale2=sub(get_scale_pair_pat, "", domain),
         domain1=sub(get_domain_pat, "", x),
         domain2=sub(get_domain_pat, "", domain)) %>%
  select(scale1, scale2, domain1, domain2, jsd)
```


Why are there two estimates here????
```{r jsd_withing_scale_plot1}
ggplot(d_practice, aes(x=domain1, y=jsd, col=scale1)) +
  geom_point(alpha=0.4) +
  facet_grid(~domain2) +
  ylab("bootstrapped jensen-shannon distance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r jsd_withing_scale_plot2}
ggplot(d_practice, aes(x=reorder(scale1, jsd), y=jsd, col=domain2)) +
  geom_point(alpha=0.4) +
  # geom_line(group=1) +
  facet_grid(~domain1) +
  ylab("bootstrapped jensen-shannon distance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# R version
```{r}
R.Version()
```

