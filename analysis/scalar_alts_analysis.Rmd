---
title: "Scalar alternatives analysis"
author: "Ben Peloquin & Mike Frank"
date: "July 22, 2016"
output:
  html_document:
    toc: true
    toc_depth: 4
---


```{r clean_session}
rm(list=ls())
```

Packages
```{r packages, warning=FALSE, message=FALSE}
library(purrr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(rjson)
library(binom)
library(lme4)
library(lmerTest)
library(rrrsa)
source("/Users/benpeloquin/Desktop/Projects/scalar_alts/analysis/scalar_alts_analysis_helpers.R")
```


Experimental set-up notes

Note: this is not a part of pre-registration analysis
```{r expt_setup}
## Literal listener trials
NUM_ITEMS <- 105
trials <- sample.int(n=NUM_ITEMS, size=NUM_ITEMS)

trial_meta <- function(n) {
  item <- n %% 21
  stars <- n %% 5
  c("item"=item, "stars"=stars)
}

d_trials <- data.frame(t(as.matrix(sapply(trials, trial_meta))))
d_trials <- d_trials %>%
  mutate(stars=stars + 1) %>%
  arrange(item)
```

# Stimuli
```{r global_helpers}
## global data
NUM_ITEMS <- 21 ## number of individual target scalars (some are shared)
DOMAINS <- c("album", "book", "game", "play", "movie", "restaurant")
## scales
bad_terrible <-            c("terrible", "bad", "okay", "good", "great")
liked_loved <-             c("hated", "disliked", "tolerated", "liked", "loved")
good_excellent <-          c("terrible", "bad", "average", "good", "excellent")
memorable_unforgettable <- c("forgettable", "boring", "ordinary", "memorable", "unforgettable")
special_unique <-          c("boring", "different", "common", "special", "unique")
all_scales <- data.frame("bad_terrible"=bad_terrible,
                         "liked_loved"=liked_loved,
                         "good_excellent"=good_excellent,
                         "memorable_unforgettable"=memorable_unforgettable,
                         "special_unique"=special_unique)
weaker <- c("bad", "liked", "good", "memorable", "special")
stronger <- c("terrible", "loved", "excellent", "unforgettable", "unique")

## valence
map_word_type <- function(word) {
  hi2 <- c("great", "loved", "excellent", "unforgettable", "unique")
  hi1 <- c("good", "liked", "memorable", "special")
  mid <- c("okay", "tolerated", "average", "ordinary", "common")
  low1 <- c("bad", "disliked", "bad", "forgettable", "different")
  low2 <- c("terrible", "hated", "terrible", "boring", "boring")
  
  if (word %in% hi2) "hi2"
  else if (word %in% hi1) "hi1"
  else if (word %in% mid) "mid"
  else if (word %in% low1) "low1"
  else if (word %in% low2) "low2"
  else "training"
}
```

# 1) Literal Listener (L0) analysis 

## Experimental set-up

Literal listener studies provide approximate literal semantic data for our target scalar items. 

We're testing 5 scales, each containing 5 items, across 6 domains (see `Stimuli` section above). Since some words are shared between scales we have 21 unique words. See `Stimuli` section above.

Data for domains is between subjects -- subjects only provide responses in the context of one domain at a time.

Data for individual words is within subjects -- subjects provide five ratings for each word (one judgment for each star rating 1-5 stars for a total of 5 * 21 = 105 responses for each participant).

Response is a binary `yes`/`no` response if the target word is compatible with the displayed star-rating.

<br>
<center>
  <br>
  <img src="/Users/benpeloquin/Desktop/Projects/scalar_alts/pre-registration/L0_design.png"
  width="500px" height="200px" style='border:1px solid #000000'>
  <br>
  <caption>Example trial from L0 Literal listener study.</caption>
  <br>
</center>
<br>

## Data pre-processing (combine domains, data-typing).
```{r L0_data_preprocessing, warning=FALSE, message=FALSE}
d_album_L0 <- create_domain_df("album", dir_path=create_dir_path("album", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)
d_book_L0 <- create_domain_df("book", dir_path=create_dir_path("book", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)
d_game_L0 <- create_domain_df("game", dir_path=create_dir_path("game", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)
d_movie_L0 <- create_domain_df("movie", dir_path=create_dir_path("movie", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)
d_play_L0 <- create_domain_df("play", dir_path=create_dir_path("play", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)
d_restaurant_L0 <- create_domain_df("restaurant",
                                 dir_path=create_dir_path("restaurant", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)

d_agg_L0 <- rbind(d_album_L0, d_book_L0, d_game_L0, d_movie_L0, d_play_L0, d_restaurant_L0)
```

```{r L0_data_prep, warning=FALSE, message=FALSE}
## Literal semantics
d_agg_sems <- d_agg_L0 %>%
  group_by(domain, item, word_type, stars) %>%
  summarise(avg_judgment=mean(judgment),
            ci_low=binom.confint(sum(judgment), length(judgment))$lower[3],
            ci_high=binom.confint(sum(judgment), length(judgment))$upper[3]) %>%
  ungroup %>%
  mutate(domain=as.factor(domain))
```

## Primary analysis - domain level differences

### Plots by scalar pairs and domains

Plot non-normalized literal listener semantics, facet by word, color by domain.
```{r L0_domain_differences_plot}
ggplot(d_agg_sems, aes(x=stars, y=avg_judgment, col=domain)) +
  geom_line() +
  geom_errorbar(aes(ymax=ci_high, ymin=ci_low), width=0) +
  facet_wrap(~item)
```

Plot non-normalized literal listener semantics, facet by scale and word-valence, color by word.
```{r L0_domain_differences_plot2}
ggplot(d_agg_sems, aes(x=stars, y=avg_judgment, col=item)) +
  geom_line() +
  geom_errorbar(aes(ymax=ci_high, ymin=ci_low), width=0) +
  facet_grid(domain~word_type)
```

Doesn't look like there's much differentiation in literal semantics by domain.

### Does domain predict response?

Statistical test: Logistic regression

Is `domain` a sigificant predictor of response.

Null hypothesis is there are no differences between domain (i.e. domain should not significantly predict response after controlling for `target_word`).
```{r L0_domain_differences_test}

## L0 listener data for glmer and glm
d_agg_L0_test <- d_agg_L0 %>%
  mutate(worker_id=as.factor(worker_id),
         study=as.factor(study),
         domain=as.factor(domain),
         word_type=as.factor(word_type),
         item=as.character(item))
# knitr::kable(summary(glm(judgment~item+domain+word_type, family="binomial", data=d_agg_L0_test)))
summary(glmer(judgment ~ item + (1|worker_id), family="binomial", data=d_agg_L0_test))
summary(glm(judgment ~ item*domain + stars, family="binomial", data=d_agg_L0_test))
```

## Secondary analysis -- word level differences

### Plots by word and domains (line chart, facets by word, lines are literal semantics for each domain)
```{r L0_word_sensitivity_plot}
# ggplot()
```

### Statistical test

- Logistic regression on data subset for each word

H0: domain should not predict response.
```{r L0_word_sensitivity_test1}
# glm()
```

- Chi-square test of independence
```{r L0_word_sensitivity_test2}
# chisq.test()
```

# 2) Pragmatic Listener (L1) analysis 

## Experimental set-up

Pragmatic listener studies provide pragmatic interpretations for our target scalar items. 

We're testing 5 scales, each containing 5 items, across 6 domains (see `stimuli` section above). Since some words are shared between scales we have 21 unique words.

Data for domains is between subjects -- subjects only provide responses in the context of one domain at a time.

Data for individual words is within subjects -- subjects provide one rating for each word (providing 1 * 21 = 21 responses per particpant).

Response is a rating between 1-5 stars.

<br>
<center>
  <br>
  <img src="/Users/benpeloquin/Desktop/Projects/scalar_alts/pre-registration/L1_design.png"
  width="500px" height="200px" style='border:1px solid #000000'>
  <br>
  <caption>Example trial from L1 Pragmatic listener study.</caption>
  <br>
</center>
<br>


## Data pre-processing (combine domains, data-typing).
```{r L1_data_preprocessing, warning=FALSE, message=FALSE}
d_album_L1 <- create_domain_df("album", dir_path=create_dir_path("album", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)
d_book_L1 <- create_domain_df("book", dir_path=create_dir_path("book", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)
d_game_L1 <- create_domain_df("game", dir_path=create_dir_path("game", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)
d_movie_L1 <- create_domain_df("movie", dir_path=create_dir_path("movie", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)
d_play_L1 <- create_domain_df("play", dir_path=create_dir_path("play", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)
d_restaurant_L1 <- create_domain_df("restaurant",
                                 dir_path=create_dir_path("restaurant", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)

d_agg_L1 <- rbind(d_album_L1, d_book_L1, d_game_L1, d_movie_L1, d_play_L1, d_restaurant_L1)
```

```{r L1_data_prep}
d_agg_prags <-  d_agg_L1 %>%
  group_by(domain, word_type, item, judgment) %>%
  summarise(counts=n())
```

## Primary analysis -- domain level differences

### Plots by scalar pairs and domains

```{r L1_domain_differences_plot}
ggplot(d_agg_prags, aes(x=judgment, y=counts, fill=domain)) +
    geom_bar(position="dodge", stat="identity") +
    facet_wrap(~item)
```

```{r}
ggplot(d_agg_prags, aes(x=judgment, y=counts, fill=item)) +
    geom_bar(position="dodge", stat="identity") +
    facet_grid(domain~word_type)
```


### Does domain predict response?

Statistical test: Multiple regression

Is `domain` a sigificant predictor of response.

Null hypothesis is there are no differences between domain (i.e. domain should not significantly predict response).
```{r L1_domain_differences_test}
summary(lmer(judgment~domain + (1|worker_id) + (1|item), data=d_agg_L1))
```

## Secondary analysis -- word level differences

### Plots by word and domains (line chart, facets by word, lines are pragmatic judgments for each domain)
```{r L1_word_sensitivity_plot}
# ggplot()
```

### Statistical tests

Null hypothesis is scalar word pragmatic judgment distributions should not differ by domain.

- Multiple regression on data subset

Domain should not predict response when controlling for `target_word`
```{r L1_word_sensitivity_test1}
# glm()
```

We don't expect to see any statistical differences.
```{r L1_word_sensitivity_test2}
# chisq.test()
```


# 3) RSA comparison with human judgments

```{r rsa_read_df}

## rsa_read_df()
## ------------
## Write a data frame that is read to run through rsa.runDf
##
rsa_read_df <- function(d_L0, d_L1) {
  
  ## Normalize L0
  normalize_semantics <- function(d) {
    norm_vec(d$avg_judgment)
  }
  pattern <- "V([1-5])"
  d_L0_sems <- plyr::ddply(.data=d_L0, .variables=c("domain", "word_type", "item"), .fun=normalize_semantics) %>%
    gather(stars, sems, -c(domain, word_type, item)) %>%
    arrange(domain, item, stars, word_type) %>%
    rowwise %>%
    mutate(stars=as.numeric(gsub(pattern, "\\1", stars)))
  
  ## Combine L0 and L1 data
  d_agg_L0_merge <- d_L0_sems
  d_agg_L1_merge <- d_L1 %>%
    mutate(stars=judgment) %>%
    select(domain, word_type, item, stars, counts)

  ## Joined data (L1 not normalized)
  d_agg_join <- plyr::join(d_agg_L0_merge, d_agg_L1_merge, by=c("domain", "word_type", "item", "stars"))
  d_agg_join[is.na(d_agg_join)] <- 0
  
  ## Normalize L1
  normalize_pragmatics <- function(d) {
    norm_vec(d$counts)
  }
  pattern <- "V([1-5])"
  normed_prags <- plyr::ddply(.data=d_agg_join, .variables=c("domain", "word_type", "item"), .fun=normalize_pragmatics) %>%
    gather(key=stars, value=prags, c(V1, V2, V3, V4, V5)) %>%
    rowwise %>%
    mutate(stars=as.numeric(gsub(pattern, "\\1", stars))) %>%
    arrange(domain, item, stars)
  
  ## Combined normalized semantics and pragmatics df
  d_agg_final <- plyr::join(d_agg_join, normed_prags, by=c("domain", "word_type", "item", "stars"))
  
  ## Data in wide format
  sems_wide <- d_agg_final %>% select(domain, word_type, item, stars, sems) %>%
    spread(stars, sems)
  prags_wide <- d_agg_final %>% select(domain, word_type, item, stars, prags) %>%
    spread(stars, prags)
  
  ## Data holds for separate semantics and pragmatics
  sems_scales_df <- data.frame(domain=c(),
                            item=c(),
                            scale=c(),
                            "1"=c(),
                            "2"=c(),
                            "3"=c(),
                            "4"=c(),
                            "5"=c())
  prags_scales_df <- data.frame(domain=c(),
                            item=c(),
                            scale=c(),
                            "1"=c(),
                            "2"=c(),
                            "3"=c(),
                            "4"=c(),
                            "5"=c())
  ## Populate data holds
  for (domain in DOMAINS) {
    for (scale in names(all_scales)) {
      for (item in all_scales[[scale]]) {
        # print("item:")
        # print(item)
        # print("----------------------")
        curr_sems_row <- sems_wide[which(sems_wide$item==item & sems_wide$domain==domain),] %>%
          mutate(scale=scale)
        curr_prags_row <- prags_wide[which(prags_wide$item==item & prags_wide$domain==domain),] %>%
          mutate(scale=scale)
        sems_scales_df <- rbind(sems_scales_df, curr_sems_row)
        prags_scales_df <- rbind(prags_scales_df, curr_prags_row)
      }
    }
  }
  sems_scales_df <- sems_scales_df %>%
    arrange(domain, scale, word_type, item) %>%
    gather(stars, sems, -c(domain, scale, word_type, item))
  prags_scales_df <- prags_scales_df %>%
    arrange(domain, scale, word_type, item) %>%
    gather(stars, prags, -c(domain, scale, word_type, item))
  
  ## Quick check
  nrow(sems_scales_df) == nrow(prags_scales_df)
  
  sems_scales_df[1:10,]
  prags_scales_df[1:10,]
  
  d_rsa_df <- plyr::join(sems_scales_df, prags_scales_df)
  
  d_rsa_df
}

```

Get model indices
```{r}
## get_compare_indices()
## ----------------------
## map word valence labels by model type
##
get_compare_indices <- function(d, model=NA) {
  
  if (model=="2") {
    indices <- which((d$scale=="bad_terrible" &
                        (d$word_type=="low1" |  d$word_type=="low2")) |
                       (d$scale!="bad_terrible" &
                          (d$word_type=="hi1" |  d$word_type=="hi2")))
  } else if (model=="3") {
    indices <- which((d$scale=="bad_terrible" &
                        (d$word_type=="low1" |  d$word_type=="low2" | d$word_type=="hi1")) |
                       (d$scale!="bad_terrible" &
                          (d$word_type=="hi1" |  d$word_type=="hi2" | d$word_type=="low1")))
  } else if (model=="4") {
    indices <- which((d$scale=="bad_terrible" &
                        (d$word_type=="low1" |  d$word_type=="low2" | d$word_type=="hi1" | d$word_type=="hi2")) |
                       (d$scale!="bad_terrible" &
                          (d$word_type=="hi1" |  d$word_type=="hi2" | d$word_type=="low1" | d$word_type=="low2")))
    
  } else if (model=="5") {
    indices <- seq(nrow(d))
  } else error("Please specify model; one of '2', '3', '4', or '5'")
  
  indices
}
```

Combined df with sems and prags
```{r}
d_joined_m <- rsa_read_df(d_agg_sems, d_agg_prags)
```

Two-alt model
```{r create_model_dfs, warning=FALSE, message=FALSE}
two_alt_m <- d_joined_m[get_compare_indices(d_joined_m, model="2"), ]
three_alt_m <- d_joined_m[get_compare_indices(d_joined_m, model="3"), ]
four_alt_m <- d_joined_m[get_compare_indices(d_joined_m, model="4"), ]
five_alt_m <- d_joined_m[get_compare_indices(d_joined_m, model="5"), ]
```

-- Using L0 literal semantics as input to RSA how well can we fit the L1 pragmatic listener judgments varying the alternative sets available to the model?

## Basic RSA

Aggregate literal listener semantics over domains
```{r}

head(d_agg_L0)

## Combine across domains
d_L0 <- d_agg_L0 %>%
  group_by(item, word_type, stars) %>%
  summarise(avg_judgment=mean(judgment),
            ci_low=binom.confint(sum(judgment), length(judgment))$lower[3],
            ci_high=binom.confint(sum(judgment), length(judgment))$upper[3])

sems_wide <- d_L0 %>% select(word_type, item, stars, avg_judgment) %>%
    spread(stars, sems)



ggplot(d_L0, aes(x=stars, y=avg_judgment, col=word_type)) +
  geom_line() +
  geom_errorbar(aes(ymax=ci_high, ymin=ci_low), width=0)
```






Tune models aggregating over `domain`
```{r eval=FALSE}
## tune_alpha_depth()
## ------------
## tune alphas for model against taregt indices
##
tune_alpha_depth <- function(d_model,
                             tune_indices=seq(nrow(d_model)),
                             alphas=seq(1, 4, by=0.1),
                             depths=seq(1,3)) {
  cors <- data.frame(cor=c(), alpha=c(), depth=c())
  for (alpha in alphas) {
    for (depth in depths) {
      
      ## Run on each alpha/depth
      out <- d_model %>%
        split(list(.$domain, .$scale)) %>%
        map_df(~rsa.runDf(
          data=.x,
          quantityVarName="stars",
          semanticsVarName="sems",
          itemVarName="item",
          depth=depth,
          alpha=alpha))
      
      out_check <- out[tune_indices,]
      curr_cor <- cor(out_check$preds, out_check$prags)
      cors <- rbind(cors, data.frame(cor=curr_cor, alpha=alpha, depth=depth)) 
    }
  }
  cors
}
```

Two-alt model tuning
```{r eval=FALSE}
two_alt_res <- tune_alpha_depth(two_alt_m, alphas=seq(0, 6, by=0.2))
best_two_alpha <- two_alt_res[which.max(two_alt_res$cor), ]
```

Three-alt model tuning
```{r eval=FALSE}
compare_indices <- get_compare_indices(three_alt_m, model="2")
three_alt_res <- tune_alpha_depth(three_alt_m, tune_indices=compare_indices, depths=seq(1, 3))
best_three_alpha <- three_alt_res[which.max(three_alt_res$cor), ]
```

Four-alt model tuning
```{r eval=FALSE}
compare_indices <- get_compare_indices(four_alt_m, model="2")
four_alt_res <- tune_alpha_depth(four_alt_m, tune_indices=compare_indices, depths=seq(1, 3))
best_four_alpha <- four_alt_res[which.max(four_alt_res$cor), ]
```

Five-alt model tuning
```{r eval=FALSE}
compare_indices <- get_compare_indices(five_alt_m, model="2")
five_alt_res <- tune_alpha_depth(five_alt_m, tune_indices=compare_indices, depths=seq(1, 3))
best_five_alpha <- five_alt_res[which.max(five_alt_res$cor), ]
```


5-alt model, scale specific tuning
```{r aggregate_tune_over_domain}
depth <- 1
alpha <- 1

generic_tune <- function(data, curr_scale, alphas=seq(0, 4, by=0.2), depths=seq(1,3)) {
  compare_indices <- get_compare_indices(data, model="2")
  cor_store <- data.frame(alpha=c(), depth=c(), r=c())
  
  for (alpha in alphas) {
    for (depth in depths) {
      curr_run <- data %>%
        filter(curr_scale==scale) %>%
        split(list(.$domain)) %>%
        map_df(~rsa.runDf(
          data=.x,
          quantityVarName="stars",
          semanticsVarName="sems",
          itemVarName="item",
          depth=depth,
          alpha=alpha)) %>%
        slice(compare_indices)
      
      cor_store <- rbind(cor_store, data.frame(alpha=alpha, depth=depth, r=cor(curr_run$prags, curr_run$preds)))
    }
  }
  cor_store
}
ge_res <- generic_tune(data=five_alt_m, curr_scale="good_excellent", alphas=seq(0, 5, by=0.2), depths=seq(0,5))
ge_res[which.max(ge_res$r),]
ll_res <- generic_tune(data=five_alt_m, curr_scale="liked_loved", alphas=seq(0, 5, by=0.2), depths=seq(0,5))
ll_res[which.max(ll_res$r),]
mu_res <- generic_tune(data=five_alt_m, curr_scale="memorable_unforgettable", alphas=seq(0, 8, by=0.2), depths=seq(0,5))
mu_res[which.max(mu_res$r),]
bt_res <- generic_tune(data=five_alt_m, curr_scale="bad_terrible", alphas=seq(0, 5, by=0.2), depths=seq(0,5))
bt_res[which.max(bt_res$r),]
su_res <- generic_tune(data=five_alt_m, curr_scale="special_unique", alphas=seq(0, 5, by=0.2), depths=seq(0,5))
su_res[which.max(su_res$r),]
```

```{r}
plot_tuning_curves <- function(d) {
  ggplot(d, aes(x=alpha, y=r, col=as.factor(depth))) +
    geom_line()
}
plot_tuning_curves(ge_res)
plot_tuning_curves(ll_res)
plot_tuning_curves(mu_res)
plot_tuning_curves(bt_res)
plot_tuning_curves(su_res)
```


Combine all domains and just look at scales
```{r}

```


## Investigating wonkiness in plane ride back from CogSci 7.14.16
```{r eval=FALSE}
plot_matrix <- function(m, check_words) {
  df <- m %>%
    as.data.frame %>%
    mutate(stars=seq(1,5)) %>%
    gather(words, sems, -c(stars)) %>%
    filter(words %in% check_words)
  
  ggplot(df, aes(x=stars, y=sems, col=words)) +
    geom_line()
}

p1 <- five_alt_m %>%
  filter(domain=="album", scale=="liked_loved")



d_joined_rsa <- d_joined_m %>%
  split(list(.$domain, .$scale)) %>%
  map_df(~rsa.runDf(
    data=.x,
    quantityVarName="stars",
    semanticsVarName="sems",
    itemVarName="item",
    depth=2,
    alpha=3))

d_joined_rsa %>%
  filter(scale=="liked_loved",
         item %in% c("liked", "loved")) %>%
  select(prags, preds) %>%
  cor
  

all_pf %>%
  mutate(stars=as.character(stars)) %>%
  split(list(.$exp, .$scale)) %>%
  map_df(~rsa.runDf(data=.x,
         quantityVarName="stars",
         semanticsVarName="speaker.p",
         itemVarName="words"))

five_album <- five_alt_m %>% filter(domain=="album")
t_old <- plyr::ddply(peloquinFrank_5Alts,
            .variables=c("scale"),
            .fun=rsa.runDf,
            quantityVarName="stars",
            semanticsVarName="speaker.p",
            itemVarName="words")

## Using purrr
t_new <- peloquinFrank_5Alts %>%
  mutate(stars=as.character(stars))%>%
  split(.$scale) %>%
  map_df(~rsa.runDf(data=.x,
                 quantityVarName="stars",
                 semanticsVarName="speaker.p",
                 itemVarName="words"))
preds_old <- t_old$preds
preds_new <- t_new$preds
preds_old[31:40]
preds_new[31:40]
all(preds_old - preds_new < 0.001)

  
## Looking at alpha level
p1_m <- p1 %>%
  select(item, stars, sems) %>%
  spread(item, sems)
p1_m <- as.matrix(p1_m[,-1])
alpha_runs <- lapply(seq(0, 10), FUN=function(alpha) {
  rsa.reason(p1_m, alpha=alpha)
})
alpha_runs_agg <- data.frame(stars=c(), alpha=c(), disliked=c(), hated=c(), liked=c(), loved=c(), tolerated=c())
for (alpha in seq(1, length(alpha_runs))) {
  curr_alpha_df <- alpha_runs[[alpha]] %>%
    as.data.frame %>%
    mutate(alpha=alpha-1, stars=seq(1,5))
  alpha_runs_agg <- rbind(alpha_runs_agg, curr_alpha_df)  
}
alpha_runs_agg_df <- alpha_runs_agg %>%
  gather(words, sems, -c(alpha, stars))

alpha_runs_agg_df %>%
  filter(words %in% c("liked", "loved")) %>%
  ggplot(aes(x=stars, y=sems, col=as.factor(alpha))) +
    geom_line() +
    facet_wrap(~words)

## Looking at depth level
##
depth_runs <- lapply(seq(0, 5), FUN=function(depth) {
  rsa.reason(p1_m, depth=depth)
})
depth_runs_agg <- data.frame(stars=c(), depth=c(), disliked=c(), hated=c(), liked=c(), loved=c(), tolerated=c())
for (depth in seq(1, length(depth_runs))) {
  curr_depth_df <- depth_runs[[depth]] %>%
    as.data.frame %>%
    mutate(depth=depth-1, stars=seq(1,5))
  depth_runs_agg <- rbind(depth_runs_agg, curr_depth_df)  
}
depth_runs_agg_df <- depth_runs_agg %>%
  gather(words, sems, -c(depth, stars))

depth_runs_agg_df %>%
  filter(words %in% c("liked", "loved")) %>%
  ggplot(aes(x=stars, y=sems, col=as.factor(depth))) +
    geom_line() +
    facet_wrap(~words)
```


Tuned model runs
```{r eval=FALSE}
best_two_alt_preds <- plyr::ddply(two_alt_m,
                     .fun=rsa.runDf,
                     .variables=c("domain", "scale"),
                     quantityVarName="stars",
                     semanticsVarName="sems",
                     itemVarName="item",
                     alpha=best_two_alpha$alpha) %>%
  mutate(model="2")
best_three_alt_preds <- plyr::ddply(three_alt_m,
                     .fun=rsa.runDf,
                     .variables=c("domain", "scale"),
                     quantityVarName="stars",
                     semanticsVarName="sems",
                     itemVarName="item",
                     alpha=best_three_alpha$alpha) %>%
  mutate(model="3")
best_four_alt_preds <- plyr::ddply(four_alt_m,
                     .fun=rsa.runDf,
                     .variables=c("domain", "scale"),
                     quantityVarName="stars",
                     semanticsVarName="sems",
                     itemVarName="item",
                     alpha=best_four_alpha$alpha) %>%
  mutate(model="4")
best_five_alt_preds <- plyr::ddply(five_alt_m,
                     .fun=rsa.runDf,
                     .variables=c("domain", "scale"),
                     quantityVarName="stars",
                     semanticsVarName="sems",
                     itemVarName="item",
                     alpha=best_five_alpha$alpha) %>%
  mutate(model="5")

best_preds_agg <- rbind(best_two_alt_preds,
                        best_three_alt_preds,
                        best_four_alt_preds,
                        best_five_alt_preds)

compare_indices <- get_compare_indices(best_preds_agg, model="2")
best_preds_agg <- best_preds_agg[compare_indices, ] %>%
  mutate(entailment_type=ifelse(item %in% weaker, "low", "high"))

best_preds_agg %>%
  filter(scale=="good_excellent") %>%
  gather(type, value, c(prags, preds)) %>%
  ggplot(aes(x=as.numeric(stars), y=value, col=entailment_type, lty=type)) +
    geom_line() +
    facet_grid(model~domain)
peloquinFrank_5Alts %>% filter(scale=="good_excellent") %>% head(n=20)
best_preds_agg %>% filter(domain=="restaurant", scale=="good_excellent", model=="5") %>% arrange(item)

ge_rest5 <- five_alt_m %>% filter(domain=="restaurant", scale=="good_excellent")
ge_rest5_preds <- plyr::ddply(ge_rest5, .variables=c("scale"), .fun=rsa.runDf,
            quantityVarName="stars", itemVarName="item", semanticsVarName="sems", alpha=5.2) %>% arrange(item)
ge_rest5_preds %>%
  gather(type, value, c(preds, prags)) %>%
  filter(word_type=="hi1" | word_type=="hi2") %>%
  ggplot(aes(x=as.numeric(stars), y=value, lty=type, col=word_type)) +
    geom_line()

ge_rest5_preds %>%
  filter(word_type=="hi1" | word_type=="hi2") %>%
  select(item, prags, preds)
cor(ge_rest5_preds$preds, ge_rest5_preds$prags)

ge_original <- plyr::ddply(.data=peloquinFrank_5Alts, .variables=c("scale"), .fun=rsa.runDf,
          quantityVarName="stars", semanticsVarName="speaker.p", itemVarName="words", alpha=3.8) %>%
  filter(scale=="good_excellent", (words=="good" | words=="excellent"))

ge_original %>%
  gather(type, value, c(preds, e11)) %>%
  ggplot(aes(x=as.numeric(stars), y=value, lty=type, col=words)) +
    geom_line()

ge_m1 <- peloquinFrank_5Alts %>%
  filter(scale=="good_excellent") %>%
  select(stars, words, speaker.p) %>%
  spread(words, speaker.p) %>% as.matrix

ge_m2_preds <- five_alt_m %>% filter(domain=="restaurant", scale=="good_excellent") %>%
  select(stars, item, sems) %>%
  mutate(stars=as.numeric(stars)) %>%
  spread(item, sems) %>% as.matrix

ge_m2_prags <- five_alt_m %>% filter(domain=="restaurant", scale=="good_excellent") %>%
  select(stars, item, prags) %>%
  mutate(stars=as.numeric(stars)) %>%
  spread(item, prags) %>% as.matrix


### This is interesting to work off of!!!
## --------------------------------------
ge_m2_preds1 <- rsa.reason(m=ge_m2_preds[,-c(1)], depth=2, alpha=3.9) %>%
  as.data.frame %>%
  mutate(stars=row.names(.)) %>%
  gather(item, preds, -c(stars))

ge_m2_preds1 %>%
  filter(item=="good" | item=="excellent") %>%
  ggplot(aes(x=as.numeric(stars), y=preds, col=item)) +
    geom_line()

ge_join <- plyr::join(ge_m2_prags, ge_m2_preds1)
cor(ge_join$prags, ge_join$preds)
```


```{r models, eval=FALSE}

curr_domain <- "book"

two_alt_m_book <- two_alt_m %>% filter(domain==curr_domain)
three_alt_m_book <- three_alt_m %>% filter(domain==curr_domain)
four_alt_m_book <- four_alt_m %>% filter(domain==curr_domain)
five_alt_m_book <- five_alt_m %>% filter(domain==curr_domain)


get_compare_indices(d) {
  which((d$scale == "bad_terrible" &
           (d$item == "" | d$item == "")) | 
          (d$scale != "bad_terrible" &
              (d$word_type == "hi1" | d$word_type == "hi2")))
}
two_alt_indices <- 

rsa.tuneDepthAlpha(two_alt_m_book


two_alt_preds <- plyr::ddply(two_alt_m,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item")

three_alt_preds <- plyr::ddply(three_alt_m,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item")

four_alt_preds <- plyr::ddply(four_alt_m,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item")
cor(four_alt_preds$prags, four_alt_preds$preds)


five_alt_preds <- plyr::ddply(five_alt_m,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item",
            alpha=1)
```


What's happening with model runs
```{r eval=FALSE}
## good_excellent + restaurant
five_alt_practice_sems <- five_alt_preds %>%
  filter(domain=="restaurant", scale=="good_excellent") %>%
  select(word_type, stars, sems) %>%
  spread(word_type, sems) %>%
  select(stars, low2, low1, mid, hi1, hi2)
row.names(five_alt_practice_sems) <- five_alt_practice_sems$stars
five_alt_practice_sems <- five_alt_practice_sems[, -1]

debug(rsa.reason)
rsa.reason(as.matrix(five_alt_practice_sems))
```


```{r eval=FALSE}
model_preds_test <- function(model_preds, test=cor) {
  filtered_preds <- model_preds %>% filter(word_type=="hi1" | word_type=="hi2")
  test(filtered_preds$prags, filtered_preds$preds)
}
model_preds_test(five_alt_preds)

five_alt_preds %>%
  filter((word_type=="hi1" | word_type=="hi2")) %>%
  ggplot(aes(x=preds, y=prags, col=domain)) +
    geom_point(alpha=0.4)

str(two_alt_preds)

View(two_alt_preds)
```


-- `depth=1`, `alpha=1` 2-alt vs 3-alt vs 4-alt vs 5-alt models

-- Standard theory predicts that listeners need only consider entailment alternatives in order to generate the implicature (2-alt model)

-- Test: do we see better model fit (correlation) by adding more alternatives and tuning hyper-parameters?
```{r basic_rsa_test}

```

## 2-alt model
```{r eval=FALSE}
d_L0_scales <- d_L0_scales %>% mutate(stars=as.character(stars)) 
d_L0_scales_2alt <- d_L0_scales %>%
  mutate(stars=as.character(stars)) %>%
  filter(word_type=="hi1" | word_type=="hi2")
two_alt_preds <- plyr::ddply(d_L0_scales_2alt,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item")
```

## 3-alt model
```{r eval=FALSE}
d_L0_scales <- d_L0_scales %>% mutate(stars=as.character(stars)) 
d_L0_scales_3alt <- d_L0_scales %>%
  mutate(stars=as.character(stars)) %>%
  filter(word_type=="hi1" | word_type=="hi2" | word_type=="low1")
three_alt_preds <- plyr::ddply(d_L0_scales_3alt,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item")
```

```{r eval=FALSE}
three_alt_preds %>% View
three_alt_preds %>%
  ggplot(aes(x=stars, y=preds, fill=word_type)) +
    geom_bar(stat="identity", position="dodge") +
    facet_grid(domain~scale)
```

## 4-alt model
```{r eval=FALSE}
d_L0_scales <- d_L0_scales %>% mutate(stars=as.character(stars)) 
d_L0_scales_4alt <- d_L0_scales %>%
  mutate(stars=as.character(stars)) %>%
  filter(word_type=="hi1" | word_type=="hi2" | word_type=="low1" | word_type=="low2")
four_alt_preds <- plyr::ddply(d_L0_scales_4alt,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item")
```

```{r eval=FALSE}
d_L0_scales_5alt <- d_L0_scales %>% mutate(stars=as.character(stars)) 
five_alt_preds <- plyr::ddply(d_L0_scales_5alt,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item")
```


```{r eval=FALSE}
two_alt_preds %>% View()
three_alt_preds %>% head(n=10)
four_alt_preds %>% head(n=10)
five_alt_preds %>% head(n=25)
```

```{r eval=FALSE}
plyr::ddply(peloquinFrank_2Alts,
            .variables=c("scale"),
            .fun=rsa.runDf,
            quantityVarName="stars",
            semanticsVarName="speaker.p",
            itemVarName="words")
plyr::ddply(peloquinFrank_5Alts,
            .variables=c("scale"),
            .fun=rsa.runDf,
            quantityVarName="stars",
            semanticsVarName="speaker.p",
            itemVarName="words")

peloquinFrank_2Alts %>%
  filter(scale=="good_excellent") %>%
  spread(words, speaker.p)
```


## Tuned RSA

-- `depth=fit`, `alpha=fit` 2-alt vs 3-alt vs 4-alt vs 5-alt models

-- Standard theory predicts that listeners need only consider entailment alternatives in order to generate the implicature (2-alt model)

-- Test: do we see better model fit (correlation) by adding more alternatives?
```{r basic_rsa_plot}
# rsa.tuneDepthAlpha()
```

## Secondary analysis

-- Any particular domains are scalar families harder for RSA to fit? Why?
```{r}
# ggplot()
```
