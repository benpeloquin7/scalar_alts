---
title: "Scalar alternatives analysis"
author: "Ben Peloquin & Mike Frank"
date: "July 22, 2016"
output:
  html_document:
    toc: true
    toc_depth: 4
---


```{r clean_session}
rm(list=ls())
```

Packages
```{r packages, warning=FALSE, message=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(rjson)
library(binom)
library(lme4)
library(lmerTest)
library(rrrsa)
source("/Users/benpeloquin/Desktop/Projects/scalar_alts/analysis/scalar_alts_analysis_helpers.R")
```


Experimental set-up notes

Note: this is not a part of pre-registration analysis
```{r expt_setup}
## Literal listener trials
NUM_ITEMS <- 105
trials <- sample.int(n=NUM_ITEMS, size=NUM_ITEMS)

trial_meta <- function(n) {
  item <- n %% 21
  stars <- n %% 5
  c("item"=item, "stars"=stars)
}

d_trials <- data.frame(t(as.matrix(sapply(trials, trial_meta))))
d_trials <- d_trials %>%
  mutate(stars=stars + 1) %>%
  arrange(item)
```

# Stimuli
```{r global_helpers}
## global data
NUM_ITEMS <- 21 ## number of individual target scalars (some are shared)
DOMAINS <- c("album", "book", "game", "play", "movie", "restaurant")
## scales
bad_terrible <-            c("terrible", "bad", "okay", "good", "great")
liked_loved <-             c("hated", "disliked", "tolerated", "liked", "loved")
good_excellent <-          c("terrible", "bad", "average", "good", "excellent")
memorable_unforgettable <- c("forgettable", "boring", "ordinary", "memorable", "unforgettable")
special_unique <-          c("boring", "different", "common", "special", "unique")
all_scales <- data.frame("bad_terrible"=bad_terrible,
                         "liked_loved"=liked_loved,
                         "good_excellent"=good_excellent,
                         "memorable_unforgettable"=memorable_unforgettable,
                         "special_unique"=special_unique)

## valence
map_word_type <- function(word) {
  hi2 <- c("great", "loved", "excellent", "unforgettable", "unique")
  hi1 <- c("good", "liked", "memorable", "special")
  mid <- c("okay", "tolerated", "average", "ordinary", "common")
  low1 <- c("bad", "disliked", "bad", "boring", "different")
  low2 <- c("terrible", "hated", "terrible", "forgettable", "boring")
  
  if (word %in% hi2) "hi2"
  else if (word %in% hi1) "hi1"
  else if (word %in% mid) "mid"
  else if (word %in% low1) "low1"
  else if (word %in% low2) "low2"
  else "training"
}
```

Data processing helpers
```{r data_processing_helpers}
## create_L1_domain_df()
## ---------------------
##
# create_L1_domain_df <- function(domain=NA, dir_path=NA, filter=TRUE, verbose=TRUE) {
#   if (is.na(domain) | is.na(dir_path)) error("Need to pass `domain` and `dir_path` args")
#   
#   ## Data hold
#   d <- data.frame(worker_id=c(),
#                    study=c(),
#                    domain=c(),
#                    word_type=c(),
#                    item=c(),
#                    judgment=c(),
#                    age=c(),
#                    gender=c(),
#                    expt_aim=c(),
#                    expt_gen=c(),
#                    language=c())
#   dir_path <- paste0("/Users/benpeloquin/Desktop/Projects/scalar_alts/data/L1_data/", domain, "/")
#   
#   ## Add files
#   files <- list.files(dir_path)
#   if (verbose) warning(paste0("Currently reading in files for ", domain, "..."))
#   for (file in files) {
#     d <- rbind(d, get_data_single_file(paste0(dir_path, file), domain, study_type="L1"))
#   }
#   
#   ## Check for filtering
#   if (filter) return_df <- filter_df(d, study_type="L1", verbose=verbose)
#   else return_df <- d
#   
#   return_df
# }
# 
# ## create_L0_domain_df()
# ## ---------------------
# ##
# create_L0_domain_df <- function(domain=NA, dir_path=NA, filter=TRUE, verbose=TRUE) {
#   if (is.na(domain) | is.na(dir_path)) error("Need to pass `domain` and `dir_path` args")
#   
#   ## Data hold
#   d <- data.frame(worker_id=c(),
#                    study=c(),
#                    domain=c(),
#                    word_type=c(),
#                    item=c(),
#                    stars=c(),
#                    judgment=c(),
#                    age=c(),
#                    gender=c(),
#                    expt_aim=c(),
#                    expt_gen=c(),
#                    language=c())
#   dir_path <- paste0("/Users/benpeloquin/Desktop/Projects/scalar_alts/data/L0_data/", domain, "/")
#   
#   ## Add files
#   files <- list.files(dir_path)
#   if (verbose) warning(paste0("Currently reading in files for ", domain, "..."))
#   for (file in files) {
#     d <- rbind(d, get_data_single_file(paste0(dir_path, file), domain, study_type="L0"))
#   }
#   
#   ## Check for filtering
#   if (filter) return_df <- filter_df(d, study_type="L1", verbose=verbose)
#   else return_df <- d
#   
#   return_df
# }
# 
# ## create_domain_df() 
# ## ------------------
# ## create a single domain level df
# ##
# create_domain_df <- function(domain=NA, dir_path=NA, study_type=NA, filter=TRUE, verbose=TRUE) {
#   if (is.na(domain) | is.na(dir_path)) error("Need to pass `domain` and `dir_path` args")
#   if (is.na(study_type)) error("Pass in study type; either `L0` or `L1`")
#   
#   if (study_type == "L0") {
#     ## L0
#     return(create_L0_domain_df(domain=domain, dir_path=dir_path, filter=filter, verbose=verbose))
#   } else {
#     ## L1
#     return(create_L1_domain_df(domain=domain, dir_path=dir_path, filter=filter, verbose=verbose))
#   }
# }
```

# 1) Literal Listener (L0) analysis 

## Experimental set-up

Literal listener studies provide approximate literal semantic data for our target scalar items. 

We're testing 5 scales, each containing 5 items, across 6 domains (see `Stimuli` section above). Since some words are shared between scales we have 21 unique words. See `Stimuli` section above.

Data for domains is between subjects -- subjects only provide responses in the context of one domain at a time.

Data for individual words is within subjects -- subjects provide five ratings for each word (one judgment for each star rating 1-5 stars for a total of 5 * 21 = 105 responses for each participant).

Response is a binary `yes`/`no` response if the target word is compatible with the displayed star-rating.

<br>
<center>
  <br>
  <img src="/Users/benpeloquin/Desktop/Projects/scalar_alts/pre-registration/L0_design.png"
  width="500px" height="200px" style='border:1px solid #000000'>
  <br>
  <caption>Example trial from L0 Literal listener study.</caption>
  <br>
</center>
<br>

## Data pre-processing (combine domains, data-typing).
```{r L0_data_preprocessing}
d_album_L0 <- create_domain_df("album", dir_path=create_dir_path("album", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)
d_book_L0 <- create_domain_df("book", dir_path=create_dir_path("book", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)
d_game_L0 <- create_domain_df("game", dir_path=create_dir_path("game", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)
d_movie_L0 <- create_domain_df("movie", dir_path=create_dir_path("movie", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)
d_play_L0 <- create_domain_df("play", dir_path=create_dir_path("play", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)
d_restaurant_L0 <- create_domain_df("restaurant",
                                 dir_path=create_dir_path("restaurant", "L0"), study_type="L0", filter=TRUE, verbose=TRUE)

d_agg_L0 <- rbind(d_album_L0, d_book_L0, d_game_L0, d_movie_L0, d_play_L0, d_restaurant_L0)
```

```{r L0_data_prep}
## Literal semantics
d_agg_sems <- d_agg_L0 %>%
  group_by(domain, item, word_type, stars) %>%
  summarise(avg_judgment=mean(judgment),
            ci_low=binom.confint(sum(judgment), length(judgment))$lower[3],
            ci_high=binom.confint(sum(judgment), length(judgment))$upper[3]) %>%
  ungroup %>%
  mutate(domain=as.factor(domain))
```

## Primary analysis - domain level differences

### Plots by scalar pairs and domains
```{r L0_domain_differences_plot}
ggplot(d_agg_sems, aes(x=stars, y=avg_judgment, col=domain)) +
  geom_line() +
  geom_errorbar(aes(ymax=ci_high, ymin=ci_low), width=0) +
  facet_wrap(~item)
```

```{r L0_domain_differences_plot2}
ggplot(d_agg_sems, aes(x=stars, y=avg_judgment, col=item)) +
  geom_line() +
  geom_errorbar(aes(ymax=ci_high, ymin=ci_low), width=0) +
  facet_grid(domain~word_type)
```

### Does domain predict response?

Statistical test: Logistic regression

Is `domain` a sigificant predictor of response.

Null hypothesis is there are no differences between domain (i.e. domain should not significantly predict response after controlling for `target_word`).
```{r L0_domain_differences_test}
d_agg_L0_test <- d_agg_L0 %>%
  mutate(worker_id=as.factor(worker_id),
         item=as.factor(item))
summary(glm(judgment~item+domain, family="binomial", data=d_agg_test))
summary(glmer(judgment~domain+(1|item)+(1|worker_id), family="binomial", data=d_agg_test))
```

## Secondary analysis -- word level differences

### Plots by word and domains (line chart, facets by word, lines are literal semantics for each domain)
```{r L0_word_sensitivity_plot}
# ggplot()
```

### Statistical test

- Logistic regression on data subset for each word

H0: domain should not predict response.
```{r L0_word_sensitivity_test1}
# glm()
```

- Chi-square test of independence
```{r L0_word_sensitivity_test2}
# chisq.test()
```

# 2) Pragmatic Listener (L1) analysis 

## Experimental set-up

Pragmatic listener studies provide pragmatic interpretations for our target scalar items. 

We're testing 5 scales, each containing 5 items, across 6 domains (see `stimuli` section above). Since some words are shared between scales we have 21 unique words.

Data for domains is between subjects -- subjects only provide responses in the context of one domain at a time.

Data for individual words is within subjects -- subjects provide one rating for each word (providing 1 * 21 = 21 responses per particpant).

Response is a rating between 1-5 stars.

<br>
<center>
  <br>
  <img src="/Users/benpeloquin/Desktop/Projects/scalar_alts/pre-registration/L1_design.png"
  width="500px" height="200px" style='border:1px solid #000000'>
  <br>
  <caption>Example trial from L1 Pragmatic listener study.</caption>
  <br>
</center>
<br>


## Data pre-processing (combine domains, data-typing).
```{r L1_data_preprocessing}
d_album_L1 <- create_domain_df("album", dir_path=create_dir_path("album", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)
d_book_L1 <- create_domain_df("book", dir_path=create_dir_path("book", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)
d_game_L1 <- create_domain_df("game", dir_path=create_dir_path("game", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)
d_movie_L1 <- create_domain_df("movie", dir_path=create_dir_path("movie", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)
d_play_L1 <- create_domain_df("play", dir_path=create_dir_path("play", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)
d_restaurant_L1 <- create_domain_df("restaurant",
                                 dir_path=create_dir_path("restaurant", "L1"), study_type="L1", filter=TRUE, verbose=TRUE)

d_agg_L1 <- rbind(d_album_L1, d_book_L1, d_game_L1, d_movie_L1, d_play_L1, d_restaurant_L1)
```

```{r L0_data_prep}
d_agg_prags <-  d_agg_L1 %>%
  group_by(domain, item, judgment) %>%
  summarise(counts=n())
```

## Primary analysis -- domain level differences

### Plots by scalar pairs and domains

```{r L1_domain_differences_plot}
ggplot(d_agg_prags, aes(x=judgment, y=counts, fill=domain)) +
    geom_bar(position="dodge", stat="identity") +
    facet_wrap(~item)
```

### Does domain predict response?

Statistical test: Multiple regression

Is `domain` a sigificant predictor of response.

Null hypothesis is there are no differences between domain (i.e. domain should not significantly predict response).
```{r L1_domain_differences_test}
summary(lmer(judgment~domain + word_type + (1|worker_id) + (1|item), data=d_agg_L1))
```

## Secondary analysis -- word level differences

### Plots by word and domains (line chart, facets by word, lines are pragmatic judgments for each domain)
```{r L1_word_sensitivity_plot}
# ggplot()
```

### Statistical tests

Null hypothesis is scalar word pragmatic judgment distributions should not differ by domain.

- Multiple regression on data subset

Domain should not predict response when controlling for `target_word`
```{r L1_word_sensitivity_test1}
# glm()
```

We don't expect to see any statistical differences.
```{r L1_word_sensitivity_test2}
# chisq.test()
```


# 3) RSA comparison with human judgments

-- Using L0 literal semantics as input to RSA how well can we fit the L1 pragmatic listener judgments varying the alternative sets available to the model?

## Basic RSA

Delta function -normalize literal semantics
```{r}
norm_vec <- function(v) v / sum(v)
normalize_semantics <- function(d) {
  norm_vec(d$avg_judgment)
}
test_s <- "V5"
pattern <- "V([1-5])"
gsub(pattern, replacement = "\\1", x = test_s)
d_L0_sems <- plyr::ddply(.data=d_agg_sems, .variables=c("domain", "item"), .fun=normalize_semantics) %>%
  gather(stars, sems, -c(domain, item)) %>%
  rowwise %>%
  mutate(stars=as.numeric(gsub(pattern, "\\1", stars))) %>%
  arrange(domain, item, stars)
```


```{r}
d_L0_sems_long <- d_L0_sems %>%
  spread(stars, sems)

d_L0_scales <- data.frame(domain=c(),
                          item=c(),
                          scale=c(),
                          "1"=c(),
                          "2"=c(),
                          "3"=c(),
                          "4"=c(),
                          "5"=c())
for (domain in DOMAINS) {
  for (scale in names(all_scales)) {
    for (item in all_scales[[scale]]) {
      print("item:")
      print(item)
      print("----------------------")
      
      curr_row <- d_L0_sems_long[which(d_L0_sems_long$item==item & d_L0_sems_long$domain==domain),] %>%
        mutate(scale=scale)
      d_L0_scales <- rbind(d_L0_scales, curr_row)
      # print(d_L0_sems_long %>% filter(domain==domain, item==item) %>%
      #   mutate(scale=scale))
    }
  }
}
d_L0_scales <- d_L0_scales %>%
  mutate(scale=as.factor(scale)) %>%
  gather(stars, sems, -c(domain, item, scale)) %>%
  select(domain, scale, item, stars, sems) %>%
  rowwise %>%
  mutate(word_type=map_word_type(item)) %>%
  arrange(domain, scale, word_type, item, stars, sems)
```

Normalized semantics plot
```{r L0_plot}
d_L0_scales <- d_L0_scales %>% mutate(stars=as.numeric(stars))

d_L0_scales %>%
  ggplot(aes(x=stars, y=sems, col=item)) +
    geom_line() +
    facet_grid(scale~domain)
```

-- `depth=1`, `alpha=1` 2-alt vs 3-alt vs 4-alt vs 5-alt models

-- Standard theory predicts that listeners need only consider entailment alternatives in order to generate the implicature (2-alt model)

-- Test: do we see better model fit (correlation) by adding more alternatives and tuning hyper-parameters?
```{r basic_rsa_test}

```

## 2-alt model
```{r}
d_L0_scales <- d_L0_scales %>% mutate(stars=as.character(stars)) 
d_L0_scales_2alt <- d_L0_scales %>%
  mutate(stars=as.character(stars)) %>%
  filter(word_type=="hi1" | word_type=="hi2")
two_alt_preds <- plyr::ddply(d_L0_scales_2alt,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item")
View(two_alt_preds)
```

```{r}
two_alt_preds %>% View
two_alt_preds %>%
  ggplot(aes(x=stars, y=preds, fill=word_type)) +
    geom_bar(stat="identity", position="dodge") +
    facet_grid(domain~scale)

```

## 3-alt model
```{r}
d_L0_scales <- d_L0_scales %>% mutate(stars=as.character(stars)) 
d_L0_scales_3alt <- d_L0_scales %>%
  mutate(stars=as.character(stars)) %>%
  filter(word_type=="hi1" | word_type=="hi2" | word_type=="low1")
three_alt_preds <- plyr::ddply(d_L0_scales_3alt,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item")
```

```{r}
three_alt_preds %>% View
three_alt_preds %>%
  ggplot(aes(x=stars, y=preds, fill=word_type)) +
    geom_bar(stat="identity", position="dodge") +
    facet_grid(domain~scale)
```

## 4-alt model
```{r}
d_L0_scales <- d_L0_scales %>% mutate(stars=as.character(stars)) 
d_L0_scales_4alt <- d_L0_scales %>%
  mutate(stars=as.character(stars)) %>%
  filter(word_type=="hi1" | word_type=="hi2" | word_type=="low1" | word_type=="low2")
four_alt_preds <- plyr::ddply(d_L0_scales_4alt,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item")
```

```{r}
d_L0_scales_5alt <- d_L0_scales %>% mutate(stars=as.character(stars)) 
five_alt_preds <- plyr::ddply(d_L0_scales_5alt,
            .fun=rsa.runDf,
            .variables=c("domain", "scale"),
            quantityVarName="stars",
            semanticsVarName="sems",
            itemVarName="item")
```


```{r}
two_alt_preds %>% View()
three_alt_preds %>% head(n=10)
four_alt_preds %>% head(n=10)
five_alt_preds %>% head(n=25)
```

```{r}
plyr::ddply(peloquinFrank_2Alts,
            .variables=c("scale"),
            .fun=rsa.runDf,
            quantityVarName="stars",
            semanticsVarName="speaker.p",
            itemVarName="words")
plyr::ddply(peloquinFrank_5Alts,
            .variables=c("scale"),
            .fun=rsa.runDf,
            quantityVarName="stars",
            semanticsVarName="speaker.p",
            itemVarName="words")

peloquinFrank_2Alts %>%
  filter(scale=="good_excellent") %>%
  spread(words, speaker.p)
```


## Tuned RSA

-- `depth=fit`, `alpha=fit` 2-alt vs 3-alt vs 4-alt vs 5-alt models

-- Standard theory predicts that listeners need only consider entailment alternatives in order to generate the implicature (2-alt model)

-- Test: do we see better model fit (correlation) by adding more alternatives?
```{r basic_rsa_plot}
# rsa.tuneDepthAlpha()
```

## Secondary analysis

-- Any particular domains are scalar families harder for RSA to fit? Why?
```{r}
# ggplot()
```
