---
title: "Scalar alternatives analysis"
author: "Ben Peloquin & Mike Frank"
date: "July 22, 2016"
output:
  html_document:
    toc: true
    toc_depth: 4
---

Packages
```{r packages, warning=FALSE, message=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
```

Experimental set-up notes

Note: this is not a part of pre-registration analysis
```{r expt_setup}
## Literal listener trials
NUM_ITEMS <- 105
trials <- sample.int(n=NUM_ITEMS, size=NUM_ITEMS)

trial_meta <- function(n) {
  item <- n %% 21
  stars <- n %% 5
  c("item"=item, "stars"=stars)
}

d_trials <- data.frame(t(as.matrix(sapply(trials, trial_meta))))
d_trials <- d_trials %>%
  mutate(stars=stars + 1) %>%
  arrange(item)
```

# Stimuli
```{r data_helpers}
## global data
NUM_ITEMS <- 21 ## number of individual target scalars (some are shared)
DOMAINS <- c("album", "book", "game", "play", "movie", "restaurant")
## scales
bad_terrible <-            c("terrible", "bad", "okay", "good", "great")
liked_loved <-             c("hated", "disliked", "tolerated", "liked", "loved")
good_excellent <-          c("terrible", "bad", "average", "good", "excellent")
memorable_unforgettable <- c("forgettable", "boring", "ordinary", "memorable", "unforgettable")
special_unique <-          c("boring", "different", "common", "special", "unique")
```


# 1) Literal Listener (L0) analysis 

## Experimental set-up

Literal listener studies provide approximate literal semantic data for our target scalar items. 

We're testing 5 scales, each containing 5 items, across 6 domains (see `Stimuli` section above). Since some words are shared between scales we have 21 unique words. See `Stimuli` section above.

Data for domains is between subjects -- subjects only provide responses in the context of one domain at a time.

Data for individual words is within subjects -- subjects provide five ratings for each word (one judgment for each star rating 1-5 stars for a total of 5 * 21 = 105 responses for each participant).

Response is a binary `yes`/`no` response if the target word is compatible with the displayed star-rating.

<br>
<center>
  <br>
  <img src="/Users/benpeloquin/Desktop/Projects/scalar_alts/pre-registration/L0_design.png"
  width="500px" height="200px" style='border:1px solid #000000'>
  <br>
  <caption>Example trial from L0 Literal listener study.</caption>
  <br>
</center>
<br>

## Data pre-processing (combine domains, data-typing).
```{r L0_data_preprocessing}
# read.csv()
```

## Primary analysis - domain level differences

### Plots by scalar pairs and domains
```{r L0_domain_differences_plot}
# ggplot()
```

### Does domain predict response?

Statistical test: Logistic regression

Is `domain` a sigificant predictor of response.

Null hypothesis is there are no differences between domain (i.e. domain should not significantly predict response after controlling for `target_word`).
```{r L0_domain_differences_test}
# glm()
```

## Secondary analysis -- word level differences

### Plots by word and domains (line chart, facets by word, lines are literal semantics for each domain)
```{r L0_word_sensitivity_plot}
# ggplot()
```

### Statistical test

- Logistic regression on data subset for each word

H0: domain should not predict response.
```{r L0_word_sensitivity_test1}
# glm()
```

- Chi-square test of independence
```{r L0_word_sensitivity_test2}
# chisq.test()
```

# 2) Pragmatic Listener (L1) analysis 

## Experimental set-up

Pragmatic listener studies provide pragmatic interpretations for our target scalar items. 

We're testing 5 scales, each containing 5 items, across 6 domains (see `stimuli` section above). Since some words are shared between scales we have 21 unique words.

Data for domains is between subjects -- subjects only provide responses in the context of one domain at a time.

Data for individual words is within subjects -- subjects provide one rating for each word (providing 1 * 21 = 21 responses per particpant).

Response is a rating between 1-5 stars.

<br>
<center>
  <br>
  <img src="/Users/benpeloquin/Desktop/Projects/scalar_alts/pre-registration/L1_design.png"
  width="500px" height="200px" style='border:1px solid #000000'>
  <br>
  <caption>Example trial from L1 Pragmatic listener study.</caption>
  <br>
</center>
<br>


## Data pre-processing (combine domains, data-typing).
```{r L1_data_preprocessing}
# read.csv()
```

## Primary analysis -- domain level differences

### Plots by scalar pairs and domains
```{r L1_domain_differences_plot}
# ggplot()
```

### Does domain predict response?

Statistical test: Multiple regression

Is `domain` a sigificant predictor of response.

Null hypothesis is there are no differences between domain (i.e. domain should not significantly predict response).
```{r L1_domain_differences_test}
# glm()
```

## Secondary analysis -- word level differences

### Plots by word and domains (line chart, facets by word, lines are pragmatic judgments for each domain)
```{r L1_word_sensitivity_plot}
# ggplot()
```

### Statistical tests

Null hypothesis is scalar word pragmatic judgment distributions should not differ by domain.

- Multiple regression on data subset

Domain should not predict response when controlling for `target_word`
```{r L1_word_sensitivity_test1}
# glm()
```

We don't expect to see any statistical differences.
```{r L1_word_sensitivity_test2}
# chisq.test()
```


# 3) RSA comparison with human judgments

-- Using L0 literal semantics as input to RSA how well can we fit the L1 pragmatic listener judgments varying the alternative sets available to the model?

## Basic RSA

-- `depth=1`, `alpha=1` 2-alt vs 3-alt vs 4-alt vs 5-alt models

-- Standard theory predicts that listeners need only consider entailment alternatives in order to generate the implicature (2-alt model)

-- Test: do we see better model fit (correlation) by adding more alternatives and tuning hyper-parameters?
```{r basic_rsa_test}
# rsa.runDf_grouped()
```

## Tuned RSA

-- `depth=fit`, `alpha=fit` 2-alt vs 3-alt vs 4-alt vs 5-alt models

-- Standard theory predicts that listeners need only consider entailment alternatives in order to generate the implicature (2-alt model)

-- Test: do we see better model fit (correlation) by adding more alternatives?
```{r basic_rsa_plot}
# rsa.tuneDepthAlpha()
```

## Secondary analysis

-- Any particular domains are scalar families harder for RSA to fit? Why?
```{r}
# ggplot()
```